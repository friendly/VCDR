%\section{Multivariate responses}\label{sec:glm-multiv}
As noted in \secref{sec:loglin-multiv}, in many studies, there may be
several response variables along with one or more explanatory variables,
and it is useful to try to model some properties
of their joint distribution as well as their separate dependence on
the predictors.
In the current chapter, the case study (\secref{sec:glm-case-nmes})
of demand for medical care by the elderly provides a relevant example.
There are actually four indicators of medical care, a $2 \times 2$ set of
(office vs.\ hospital) place and (physician vs.\ non-physician) practitioner.
That case study analyzed only the office visits by physicians.

This section describes a few steps in this direction.  To provide some context,
we begin with a capsule overview of classical multivariate response models.

\ix{regression}
In the case of classical linear models with Gaussian error distributions, the
model for a univariate response, $\vec{y} = \mat{X} \vec{\beta} + \vec{\epsilon}$,
with
%$\epsilon_i \sim \mathcal{N} (0, \sigma^2)$
$\vec{\epsilon} \sim \mathcal{N} (\vec{0}, \mat{\Sigma})$,
extends quite readily to the \term{multivariate linear model} (MLM) for $q$ response variables,
$\mat{Y} = \{ \vec{y}_1, \vec{y}_2, \dots , \vec{y}_q \}$.  The MLM has the form
\begin{equation}\label{eq:mlm}
\sizedmat{Y}{n\times q} = \sizedmat{X}{n\times p} \sizedmat{B}{p\times q} +
\sizedmat{E}{n\times q}
\end{equation}
where $\mat{Y}$ is a matrix of $n$ observations on $q$ response variables;
\ix{matrices}
\ix{model matrix}
$\mat{X}$ is a model matrix with columns for $p$ regressors, typically including an initial column of 1s for the regression constant;
$\mat{B}$ is a matrix of regression coefficients, one column for each response variable; and $\mat{E}$ is a matrix of errors.

It is important to note that:
\begin{itemize*}
  \item The maximum likelihood estimator of $\mat{B}$ in the MLM is equivalent to the result of fitting $q$ separate
  univariate models for the individual responses and joining the coefficients columwise, giving
\begin{equation*}
\widehat{\mat{B}} = \{ \widehat{\vec{\beta}}_1, \widehat{\vec{\beta}}_2, \dots , \widehat{\vec{\beta}}_q \}
                  = (\mat{X}\trans \mat{X})^{-1} \mat{X}\trans \mat{Y} \period
\end{equation*}
\ix{correlation}
  \item Procedures for statistical inference (hypothesis tests, confidence intervals), however, take account of the correlations among the responses.  Multivariate tests can therefore be more powerful than separate univariate tests under
  some conditions.
\ix{generalized linear model!assumptions}
  \item A unique feature of the MLM stems from the assumption of multivariate normality of the errors,
  so that
  each row $\epsilon_i \trans$ of $\mat{E}$ is assumed to be distributed independently,
  $ \epsilon_i \trans \sim \mat{\mathcal{N}}_q (\vec{0}, \mat{\Sigma})$, where $\mat{\Sigma}_{q \times q}$
  is the error covariance matrix, constant across observations, like $\sigma^2$ in univariate models.
  Then, the conditional distributions of $\vec{y}_j \given \mat{X}$ are all
  univariate normal, all bivariate distributions, $\vec{y}_j, \vec{y}_k \given \mat{X}$ are bivariate normal,
  and any linear combination of the conditional $y$s is univariate normal.

  \item Consequently, all relationships among the $\vec{y}$s can be summarized by correlations and relationships between
  the $\vec{y}$s and $\vec{x}$s by linear regressions.  These can be visualized using
\term{data ellipse}s \citep{Friendly-etal:ellipses:2013} and hypothesis tests in the MLM can be visualized
  by ellipses
  using \term{hypothesis-error plot}s
  \citep{Friendly:07:manova,FoxFriendlyMonette:09:compstat}.
\end{itemize*}

This generality of the MLM is lost, however, when we move to multivariate response models in the non-Gaussian case.
\ix{binomial}
For binomial responses, \secref{sec:loglin-multiv} described several approaches toward a multivariate logistic
regression model that attempt to separate the marginal dependence of each $\vec{y}$ on the $\vec{x}$s
from the relationship of the association among the $\vec{y}$s on the $\vec{x}$s.
The bivariate logistic model for $(\vec{y}_1, \vec{y}_2)$,
for example, was parameterized (see \eqref{eq:eta2}) in terms of submodels for a logit for each response,
$\eta_1 = \vec{x}\trans \vec{\beta}_1$,
$\eta_2 = \vec{x}\trans \vec{\beta}_2$
and a submodel for the log odds ratio,
$\theta_{12} = \vec{x}\trans \vec{\beta}_{12}$.

\ix{count data}
The situation becomes more difficult for multivariate count data responses, because parametric approaches to
\ix{joint distribution}
their joint distribution (e.g., a multivariate Poisson distribution)
given a set of explanatory variables are computationally and analytically intractable.
\citet[\C 8]{CameronTrivedi:2013} provide a detailed description of the problems and some solutions for
the bivariate case, including bivariate Poisson, negative-binomial and hurdle models.

Consequently, only a few special cases have been worked out theoretically, and mostly for the
bivariate case. For example, \cite{King:1989} described a seemingly unrelated bivariate Poisson model
for two correlated count variables. This models the separate linear predictors for $\vec{y}_1$ and
$\vec{y}_2$ as
\begin{eqnarray*}
g(\vec{\mu}_1) & = & \vec{x}_1 \trans \vec{\beta}_1 \\
g(\vec{\mu}_2) & = & \vec{x}_2 \trans \vec{\beta}_2       \comma
\end{eqnarray*}
with the covariance between $\vec{y}_1$ and
$\vec{y}_2$ represented as $\xi$. As in the MLM, the coefficients have the same point estimates as
in equation-by-equation Poisson models.  However, there is a gain in efficiency (reduced standard errors)
resulting from a bivariate full-information maximum likelihood solution, and efficiency increases with the
covariance $\xi$ between the two count variables.

As a result, for lack of a fully general model for multivariate count data, one simple approach is to
employ a method for simultaneous estimation of the equation-by-equation coefficients, accepting some loss
of efficiency. This allows for hypothesis tests that may not be the most powerful, but provide
approximate answers to more interesting questions.  We can supplement this with separate analysis
of the dependencies among the responses, and how these vary with the explanatory variables.

In \R, the \Rpackage{VGAM} is the most general available package for analysis of multivariate response GLMs.
For multivariate count data, it provides for both Poisson and negative-binomial models.  For NB
models, the dispersion parameters $\theta_j = \alpha_j^{-1}$ can be allowed to vary with the
predictors via a GLM  of the form
$\log \theta_j = \vec{x}\trans \vec{\gamma}_j$
or can be constrained to be ``intercept-only,''
$\log \theta_j = \gamma_{0j}$,
giving separate global dispersion estimates for each response.
In the latter case, the resulting coefficients are the same as fitting a separate model for
each response using \func{glm.nb}.


