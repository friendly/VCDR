% template for a new chapter
<<echo=FALSE>>=
source("Rprofile.R")
knitrSet("ch06")
.locals$ch06 <- NULL
@

\chapter{Correspondence Analysis}\label{ch:corresp}
\input{front/vtoc06}		% visual table of contents

\chapterprelude{
Correspondence analysis provides visualizations of associations in a two-way \ctab
in a small number of dimensions.
Multiple correspondence analysis extends this technique to \nway\
tables.  Other graphical methods, including mosaic matrices and biplots
provide complementary views of \loglin models for two-way and \nway
\ctabs, but \ca methods are particularly useful for a simple visual analysis.
}
% \minitoc
% \clearpage


\section{Introduction}

\epigraph{Whenever a large sample of
chaotic elements
is taken in hand and marshalled in the order of their magnitude, an
unsuspected and most beautiful form of regularity proves to have been
latent all along.}
{Sir Francis Galton (1822--1911)}

Correspondence analysis (CA) is an exploratory technique which displays
the row and column categories in a two-way contingency table as points
in a graph, so that the positions of the points represent the
associations in the table.
Mathematically, correspondence analysis is related to the \term{biplot},
to \term{canonical correlation},
and to \term{principal component analysis}.
%See \citet{Greenacre:2007} for an accessible introduction
%and \citep{Beh:2004} for a comprehensive review.
%(see \SSSGref{8,7, 9.4, 10.3}).

This technique finds
scores for the row and column categories on a small number of
dimensions which account for the greatest proportion of the
\(\chi^2\) for association between the row and column categories,
just as principal components account for maximum variance
of quantitative variables.  But CA does more---
the scores provide a quantification of the categories,
and have the property that they maximize the correlation
between the row and column variables.   For
graphical display two or three dimensions are typically used to give
a reduced rank approximation to the data.

Correspondence analysis has a very large, multi-national literature and
was rediscovered several times in different fields and different countries.
The method, in slightly different forms, is also
discussed under the names
\term{dual scaling}, \term{optimal scaling},
\term{reciprocal averaging},
\term{homogeneity analysis},
and \term{canonical analysis of
categorical data}.

See \citet{Greenacre:84} and \citet{Greenacre:2007}
for an accessible introduction to CA methodology,
or \citet{Gifi:81,Lebart-etal:84}
for a detailed treatment of the method and its applications
from the Dutch and 
French perspectives.
\citet{GreenacreHastie:87} provide an excellent discussion of
the geometric interpretation,
while \citet{HeijdenLeeuw:85} and \citet{Heijden-etal:89}
develop some of the relations between correspondence analysis
and log-linear methods for three-way and larger tables.
Correspondence analysis is usually carried out in an exploratory,
graphical way.
\citet{Goodman:81,Goodman:85,Goodman:86} has developed related inferential models, the RC model 
(see \secref{sec:RCmodels}) and
the canonical correlation model, with close links to CA.

One simple development of CA is as follows:
For a two-way table the scores for the row categories, namely
\(\mat{X} = \{x_{im}\}\), and column categories, \(\mat{Y} = \{y_{jm}\}\), on dimension \(m = 1,
\dots , \,  M\) are derived from a (generalized) \term{singular value decomposition} of
(Pearson) residuals from independence, expressed as \(d_{ij} /  \sqrt n\), to
account for the largest proportion of the \(\chi^2\) in a small
number of dimensions.  This decomposition may be expressed as
\ix{singular value decomposition}
%
\begin{equation} \label{eq:cadij}
  \frac{d_{ij}}{\sqrt{n}} =
  \frac{n_{ij} - m_{ij}} {\sqrt {n \,  m_{ij}}} =
  \mat{X} \, \mat{D}_\lambda \, \mat{Y}\trans =
  \sum_{m=1}^M  \lambda_m \,  x_{im} \,  y_{jm}
  \comma
\end{equation}
where $m_{ij}$ is the expected frequency and
where $\mat{D}_\lambda$ is a diagonal matrix with elements
\(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_M\), and \(M
=  \min ( I-1 , \,  J-1 )\).  In \(M\) dimensions, the decomposition
\eqref{eq:cadij} is exact.
For example, an \(I \times 3\) table can be depicted exactly
in two dimensions when $I \ge 3$.  The useful result for visualization
purposes is that
a rank-\(d\) approximation in \(d\) dimensions is
obtained from the first \(d\) terms on the right side of \eqref{eq:cadij}.
The proportion of the Pearson \(\chi^2\) accounted for by this approximation is
\begin{equation*}
 n \,  \sum_m^d { \,  \lambda_m^2 } \big/  \chi^2
 \period
\end{equation*}
The quantity $\chi^2 /n = \sum_i \sum_j d_{ij}^2  / n$ is called
the total \term{inertia} and is identical to the measure of
association known as Pearson's mean-square contingency, the
square of the $\phi$ coefficient.
\ix{$\phi$ coefficient}
\ix{phi coefficient@phi ($\phi$) coefficient}
\ix{mean-square contingency coefficient}

Thus, correspondence analysis is designed to show how the data
deviate from expectation when the row and column variables are
independent, as in the sieve diagram,
association plot and mosaic display.  However,
the sieve, association and mosaic plots depict every \emph{cell} in
the table, and for large tables it may be difficult to see patterns.
Correspondence analysis shows only row and column \emph{categories}
as points in
the two (or three) dimensions which account for the greatest
proportion of deviation from independence.
The pattern of the associations can then be inferred from the positions of the
row and column points.

\section{Simple correspondence analysis}\label{sec:ca-simple}
\ixon{correspondence analysis!two-way tables}

\subsection{Notation and terminology}\label{sec:ca-notation}
Because \CA\ grew up in so many homes, the notation, formulae
and terms used to describe the method vary considerably.
The notation used here generally follows \citet{Greenacre:84,Greenacre:97,Greenacre:2007}.

The descriptions here employ the following matrix and vector definitions:
\begin{itemize}
\item $\mat{N} = \{ n_{ij} \}$ is the $I \times J$ contingency table
with row and column totals $n_{i+}$ and $n_{+j}$, respectively.
The grand total $n_{++}$ is also denoted by $n$ for simplicity.
\item $\mat{P} = \{ p_{ij} \} = \mat{N}/n$ is the matrix of joint cell
proportions,  called the \term{correspondence matrix}.
\item $\vec{r} = \sum_j p_{ij} = \mat{P} \vec{1}$ is the row margin of $\mat{P}$;
$\vec{c} = \sum_i p_{ij} = \mat{P}\trans \vec{1}$ is the column margin.
$\vec{r}$ and $\vec{c}$ are called the \emph{row masses} and \emph{column masses}.
\item $\mat{D}_r$ and $\mat{D}_c$ are diagonal matrices with $\vec{r}$
and $\vec{c}$ on their diagonals, used as weights.
\item $\mat{R} = \mat{D}_r^{-1} \mat{P} = \{ n_{ij} / n_{+j} \}$ is the matrix of
row conditional probabilities, called \emph{row profiles}.
Similarly, $\mat{C} = \mat{D}_c^{-1} \mat{P}\trans = \{ n_{ij} / n_{i+} \}$ is the matrix of
column conditional probabilities or \emph{column profiles}.
\item $\mat{S} = \mat{D}_r^{-1/2} (\mat{P} - \vec{r} \vec{c}\trans) \mat{D}_c^{-1/2}$ is the
matrix of standardized Pearson residuals from independence (denoted $d_{ij}$ in the introduction).
\end{itemize}

Two types of coordinates, $\mat{X}$, $\mat{Y}$ for the row and column categories are defined,
based on the singular value decomposition (SVD) of $\mat{S}$,
\ix{singular value decomposition}
% \begin{equation*}%\label{eq:ca-svd}
% \mat{P} = \mat{A} \mat{D}_{\lambda} \mat{B}\trans
% \end{equation*}
% where $\mat{D}_{\lambda}$ is the diagonal matrix of singular values
% \(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_M\),
% $\mat{A}$ is the $I \times M$ matrix of left singular vectors,
% normalized so that
% \( \mat{A} \mat{D}_r^{-1} \mat{A}\trans = \mat{I} \), and
% $\mat{B}$ is the $J \times M$ matrix of right singular vectors,
% normalized so that
% \( \mat{B} \mat{D}_c^{-1} \mat{B}\trans = \mat{I} \).
% Thus the columns of $\mat{A}$ and $\mat{B}$ are orthogonal in the weighted metrics
% defined by the row and column margins, $\mat{D}_r^{-1}$ and $\mat{D}_c^{-1}$,
% respectively.

\begin{equation*}%\label{eq:ca-svd}
\mat{S} = \mat{U} \mat{D}_{\lambda} \mat{V}\trans \quad \mbox{where} \quad 
 \mat{U}\trans\mat{U} = \mat{V}\trans\mat{V} = \mat{I} \comma
\end{equation*}
and $\mat{D}_{\lambda}$ is the diagonal matrix of singular values
\(\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_M\).
$\mat{U}$ is the orthonormal $I \times M$ matrix of left singular vectors,
and
$\mat{V}$ is the $J \times M$ matrix of right singular vectors.

The SVD of $\mat{S}$ is related to the eigenvalue--eigenvector decomposition of a square symmetric matrix, in that
$\mat{S}\mat{S}\trans = \mat{U} \mat{D}_{\lambda}^2 \mat{U}$ and
$\mat{S}\trans\mat{S} = \mat{V} \mat{D}_{\lambda}^2 \mat{V}$,
so the values $\lambda^2$ are the eigenvalues in both cases and the
singular vectors are the corresponding eigenvectors.  
In \ca, these eigenvalues (squares of the singular values) are
called the \term{principal inertia}s, and are the values used in the decomposition of the Pearson $\chisq$
for the dimensions, $\chisq = n \sum_m \lambda_m^2$.

\begin{description}
\ix{correspondence analysis!principal coordinates}
\ix{principal coordinates}
\item[principal coordinates:]  The coordinates of the row ($\mat{F}$) and column ($\mat{G}$) profiles
with respect to their own principal axes are defined so that the inertia along
each axis is the corresponding eigenvalue value, $\lambda_m$,
\begin{eqnarray}
%
\mat{F} & = & \mat{D}_r^{-1/2} \mat{U} \mat{D}_{\lambda}^2 \quad\mbox{scaled so that} \quad \mat{F}\trans \mat{D}_r \mat{F} = \mat{D}_{\lambda}^2 \label{eq:pcoord1} \\
\mat{G} & = & \mat{D}_c^{-1/2} \mat{V} \mat{D}_{\lambda}^2 \quad\mbox{scaled so that} \quad \mat{G}\trans \mat{D}_c \mat{G} = \mat{D}_{\lambda}^2 \label{eq:pcoord2}
\end{eqnarray}
The joint plot in principal coordinates, $\mat{F}$ and $\mat{G}$, is called the
\term{symmetric map} because both row and column profiles are overlaid in the same coordinate system.

\ix{correspondence analysis!standard coordinates}
\ix{standard coordinates}
\item[standard coordinates:] The standard coordinates ($\mat{\Phi}, \mat{\Gamma}$) are a rescaling of the principal
coordinates to unit inertia along each axis,
\begin{eqnarray}
%\label{}
\mat{\Phi} & = & \mat{D}_r^{-1} \mat{U}  \quad\mbox{scaled so that} \quad \mat{\Phi}\trans \mat{D}_r \mat{\Phi} = \mat{I} \label{eq:scoord1} \\
\mat{\Gamma} & = & \mat{D}_c^{-1} \mat{V} \quad\mbox{scaled so that} \quad \mat{\Gamma}\trans \mat{D}_c \mat{\Gamma} = \mat{I} \label{eq:scoord2}
\end{eqnarray}
These differ from the principal coordinates in \eqref{eq:pcoord1}
and \eqref{eq:pcoord2} simply by the absence of the scaling factors,
$\mat{D}_{\lambda}^2$. An \term{asymmetric map} shows one set of points (say, the rows) in principal coordinates
and the other set in standard coordinates.
\end{description}
Thus, the weighted average of the squared principal coordinates
for the rows or columns on a principal axis equals the squared
singular value, $\lambda^2$ for that axis,
whereas the weighted average of the squared standard coordinates
equals 1.
The relative positions of the row or column points along any axis
is the same under either scaling,
but the distances between points differ, because the axes are
weighted differentially in the two scalings.
% The symmetric map in principal coordinates tends to be most widely used in practice
% because the row and column points are equally spread over the plotting region.

\subsection{Geometric and statistical properties}\label{sec:ca-properties}
\ixon{correspondence analysis!properties}
We summarize here some geometric and statistical properties of the
\CA\ solutions which are useful in interpretation.

\begin{description}
\item[nested solutions:] Because they use successive terms of the SVD
  \eqref{eq:cadij}, \ca solutions are \emph{nested}, meaning that the first
  two dimensions of a three-dimensional solution will be identical
  to the two-dimensional solution.

\item[centroids at the origin:] In both principal coordinates and standard
coordinates the points representing the row and column profiles have their
centroids (weighted averages) at the origin.
Thus, in CA plots, the origin represents the (weighted) average
row profile and column profile.

\item[reciprocal averages:]
CA assigns scores to the row and column categories such that
the column scores are proportional to the weighted averages of the row
scores, and vice-versa.

\item[chi-square distances:]  In principal coordinates, the row coordinates
may be shown equal to the row profiles $\mat{D}_r^{-1} \mat{P}$, rescaled 
by the inverse by the square-root of the column masses, $\mat{D}_c^{-1/2}$.
Distances between two row profiles, $\mat{R}_i$ and $\mat{R}_{i^\prime}$
are most sensibly defined as $\chi^2$ distances, where the squared
difference $[\mat{R}_{ij} -\mat{R}_{i^\prime j}]^2$ is inversely weighted
by the column frequency, to account for the different relative
frequency of the column categories.
The rescaling by $\mat{D}_c^{-1/2}$ transforms this weighted $\chi^2$
metric into ordinary Euclidean distance.
The same is true of the column principal coordinates.

\item[interpretation of distances:]
In principal coordinates,
the distance between two row points may be interpreted as described
above, and so may the distance between two column points.
The distance between a row and column point, however, does not have
a clear distance interpretation.

\item[residuals from independence:]
The distance between a row and column point do have a rough
interpretation in terms of residuals or the difference between
observed and expected frequencies, $n_{ij} - m_{ij}$.
Two row (or column) points deviate from the origin (the average
profile) when their profile frequencies have similar values.
A row point appears in a similar direction away from the origin as
a column point when  $n_{ij} - m_{ij} >
0$, and in an opposite different direction from that column point when the residual is negative.
\end{description}

Because of these differences in interpretations of distances, there
are different possibilities for graphical display.
A joint display of principal coordinates for the rows and standard
coordinates for the columns (or vice-versa), sometimes called
an \term{asymmetric map} is suggested by
\ix{correspondence analysis!asymmetric map}
\citet{GreenacreHastie:87} and by \citet{Greenacre:89} as the plot
with the most coherent geometric interpretation
(for the points in principal coordinates) and is sometimes
used in the French literature.

\ix{correspondence analysis!symmetric map}
Another common joint display is the \term{symmetric map} of the principal
coordinates in the same plot.  This is the default
in the \Rpackage{ca} described below.
In the authors' opinion, this produces better graphical displays, because
both sets of coordinates are scaled with the same weights for each axis.
Symmetric plots are used exclusively in this book, but that should
not imply that these plots are universally preferred.
Another popular choice is to avoid the possibility of misinterpretation
by making separate plots of the row and column coordinates.
%The different scalings, and the valid distance interpretations for each
%are described in detail in the Algorithms section of
%\STUGref{19}{The CORRESP Procedure}.
\ixoff{correspondence analysis!properties}

\subsection{\R software for correspondence analysis}\label{sec:ca-R}

\CA methods for computation and plotting are available in a number of
\R packages including:
\begin{description*}
  \item \pkg{MASS}: \func{corresp}; the plot method calls \func{biplot} for a 2 factor solution, using a 
  a symmetric biplot factorization that scales the row and column points by the square roots of the
  the singular values.  
  There is also a \func{mca} function for
  multiple correspondence analysis.

  \item \pkg{ca}: \func{ca}; provides 2D plots via the \func{plot.ca} method and
  interactive (\pkg{rgl}) 3D plots via \func{plot3d.ca}.  This package is the most
  comprehensive in terms of plotting options for various coordinate types,
  plotting supplementary points (see \secref{ca:marginal}) and other features.
  It also provides \func{mjca} for multiple and joint \ca of higher-way tables.
  
  \item \pkg{FactoMineR}: \func{CA}; provides a wide variety of measures for the
  quality of the CA representation and many options for graphical display
%  \item \pkg{ade4}: \func{dudi.coa}
\end{description*}
These methods also differ in terms of the types of input they accept.  For
example, \code{MASS::corresp} handles matrices, data frames and
\class{xtabs} objects, but not \class{table} objects. 
\func{ca} is the most general, with
methods for two-way tables, matrices, data frames, and \class{xtabs} objects.
In the following,
we largely use the \Rpackage{ca}.

\begin{Example}[haireye3]{Hair color and eye color}

The script below uses the two-way table \code{haireye} from the
\data{HairEyeColor} data, collapsed over \var{Sex}.
In this table, \var{Hair} colors form the rows, and \var{Eye} colors
form the columns.  By default, \func{ca} produces a 2-dimensional
solution.  In this example, the complete, exact solution would
have $M = \min((I-1), (J-1)) = 3$ dimensions, and you could obtain this
using the argument \code{nd=3} in the call to \func{ca}.

<<ca-haireye1, R.options=list(digits=4)>>=
haireye <- margin.table(HairEyeColor, 1:2)
library(ca)
(haireye.ca <- ca(haireye))
@
In the printed output, the table labeled ``Principal inertias (eigenvalues)''
indicates that nearly 99\% of the
Pearson
\(\chi^2\) for association is accounted for by two dimensions, with
most of that attributed to the first dimension. 

The \code{summary} method for \class{ca} objects gives a more nicely formatted
display, showing a \term{scree plot} of the eigenvalues, a portion of
which is shown below.
%\DONE{Make this output look consistent with the rest of the text. Perhaps need
%a knitr chunk hook to print a subset of output.}

<<ca-haireye2, output.lines=9>>=
summary(haireye.ca)
@
\noindent The Pearson $\chisq$ for this table (given by \code{chisq.test(haireye)})
is 138.29. This value is $n$ (592) times the sum of the eigenvalues (0.2336)
shown above.

The result returned by \func{ca} can be plotted using the \func{plot.ca} method.
However, it is useful to understand that \func{ca} returns the CA solution
in terms of \emph{standard coordinates}, $\Phi$ (\code{rowcoord})
and $\Gamma$ (\code{colcoord}). We illustrate \eqref{eq:scoord1}
and \eqref{eq:scoord2} using the components of the \class{ca} object
\code{haireye.ca}.

<<ca-haireye3>>=
# standard coordinates Phi (Eqn 6.4) and Gamma (Eqn 6.5)
(Phi <- haireye.ca$rowcoord)
(Gamma <- haireye.ca$colcoord)

# demonstrate orthogonality of std coordinates
Dr <- diag(haireye.ca$rowmass)
zapsmall(t(Phi) %*% Dr %*% Phi)
Dc <- diag(haireye.ca$colmass)
zapsmall(t(Gamma) %*% Dc %*% Gamma)
@

These standard coordinates are transformed internally within
the plot function according to the \code{map} argument, which defaults to
\code{map="symmetric"}, giving principal coordinates.  The following call
to \func{plot.ca} produces \figref{fig:ca-haireye-plot}.

<<ca-haireye-plot, h=5.53, w=7, out.width='.8\\textwidth', cap='Correspondence analysis solution for the Hair color and Eye color data'>>=
op <- par(cex=1.4, mar=c(5,4,1,2)+.1)
res <- plot(haireye.ca)
par(op)
@
For use in further customizing such plots (as we will see in the next example),
the function \func{plot.ca}
returns (invisibly)%
\footnote{
This uses features incorporated in the \Rpackage{ca}, version 0.54+.
}
the coordinates for the row and column points actually plotted,
which we saved above as \code{res}:
<<ca-haireye4>>=
res
@

It is important to understand that in CA plots (and related biplots, \secref{sec:biplot}),
the interpretation of distances between points (and angles between vectors)
is meaningful.  In order to achieve this, the axes in such plots must be \emph{equated},
meaning that the two axes are scaled so that the number of data units per inch
are the same for both the horizontal and vertical axes, or an \term{aspect ratio} = 1.%
\footnote{
In base \R graphics, this is achieved with the \func{plot} option \code{asp=1}.
}
\ix{axes!equating}

The interpretation of the CA plot in \figref{fig:ca-haireye-plot} is then as follows:
\begin{itemize*}
 \item Dimension 1, accounting for nearly 90\% of the association between
 hair and eye color corresponds to dark (left) vs. light (right) on both variables.
 \item Dimension 2 largely contrasts red hair and green eyes with the remaining categories, accounting for an additional 9.5\% of the Pearson $\chi^2$.
 \item With equated axes, and a symmetric map, the distances between row points and distances between 
 column points are meaningful.  Along Dimension 1, the eye colors could be considered
 roughly equally spaced, but for the hair colors, Blond is quite different in terms
 of its frequency profile.
\end{itemize*}
\end{Example}

\begin{Example}[mental3]{Mental impairment and parents' SES}
In \exref{ex:mental1} we introduced the data set \data{Mental},
relating mental health status to parents' SES.
%\TODO{Want a sieve diagram or mosaic plot in \chref{ch:mosaic} for comparison here.}
As in \exref{ex:mental2}, we convert this to a two-way table, \code{mental.tab}
to conduct a \ca.

<<ca-mental1>>=
data("Mental", package="vcdExtra")
mental.tab <- xtabs(Freq ~ ses + mental, data=Mental)
@
We calculate the CA solution, and save the result in \code{mental.ca}:
<<ca-mental2, output.lines=9>>=
mental.ca <- ca(mental.tab)
summary(mental.ca)
@
The scree plot produced by \code{summary(mental.ca)}
shows that the association between mental health
and parents' SES is almost entirely 1-dimensional, with 94\% of
the \chisq\ ( 45.98, with 15 df) accounted for by Dimension 1.

We then plot the solution as shown below, giving \figref{fig:ca-mental-plot}.
For this example, it is useful to connect the row points and the column points
by lines, to emphasize the pattern of these ordered variables.
<<ca-mental-plot, h=4, w=6, out.width='.9\\textwidth', cap='Correspondence analysis solution for the Mental health data'>>=
op <- par(cex=1.3, mar=c(5,4,1,1)+.1)
res <- plot(mental.ca,  ylim=c(-.2, .2))
lines(res$rows, col="blue", lty=3)
lines(res$cols, col="red", lty=4)
par(op)
@

The plot of the CA scores in \figref{fig:ca-mental-plot} shows that
diagnostic mental health categories are well-aligned with Dimension 1.  The
mental health scores are approximately equally spaced, except that
the two intermediate categories are a bit closer on this dimension than
the extremes.  The
SES categories are also aligned with Dimension 1, and approximately
equally spaced, with the exception of the highest two SES categories,
whose profiles are extremely similar, suggesting that
these two categories could be collapsed.

Because both row and column categories have the same pattern on
Dimension 1, we may interpret the plot as showing that the profiles
of both variables are ordered, and their relation can be explained
as a positive association between high parents' SES and higher mental
health status of children. A mosaic display of these data
(\labref{lab:6.5}) would show a characteristic opposite corner pattern of association.

From a modeling perspective,  we might ask how strong is the evidence
for the spacing of categories noted above.  For example, we might
ask whether assigning integer scores to the levels of SES and mental
impairment provides a simpler, but satisfactory account of their association.
Questions of this type can be explored in connection with \loglin models in
\chref{ch:loglin}.
%(see \exref{ex:mental2}).

\end{Example}

\begin{Example}[victims2]{Repeat victimization}

The data set \data{RepVict} in the \Rpackage{vcd} gives a $8 \times 8$ table
(from \citet[Table 2-8]{Fienberg:80})
on repeat victimization for various crimes among respondents to a
U.S. National Crime Survey.
%\exref{ex:victims} presented mosaic displays for the data on repeat victimization, \data{RepVict}.
A special feature of this data set is that
row and column categories reflect the \emph{same} crimes, so
substantial association is expected.
Here we examine \ca results in a bit more detail
and also illustrate how to customize the
displays created by \code{plot(ca(...))}.

<<ca-victims1, output.lines=13>>=
data("RepVict", package="vcd")
victim.ca <- ca(RepVict)
summary(victim.ca)
@
The results above show that, for this $8 \times 8$ table, 7 dimensions
are required for an exact solution, of which the first two account for
64.5\% of the Pearson $\chi^2$.
The lines below illustrate that the Pearson $\chi^2$ is $n$ times the
sum of the squared singular values,
$n \sum \lambda_i^2$.

<<ca-victims2>>=
chisq.test(RepVict)
(chisq <- sum(RepVict) * sum(victim.ca$sv^2))
@

The default plot produced by \code{plot.ca(victim.ca)}
plots both points and labels for the row and column categories.
However, what we want to emphasize here is the relation between
the \emph{same} crimes on the first and second occurrence.

To do this, we  label each crime just once (using \code{labels=c(2,0)})
and connect the two points for each crime by a line,
using \func{segments}, as shown in \figref{fig:ca-victims-plot}.
The addition of a \func{legend} makes the plot more easily readable.

<<ca-victims-plot, h=6, w=6, out.width='.65\\textwidth', cap='2D CA solution for the repeat victimization data. Lines connect the category points for first and second occurrence to highlight these relations.', fig.pos='!htb'>>=
op <- par(cex=1.3, mar=c(4,4,1,1)+.1)
res <- plot(victim.ca, labels=c(2,0))
segments(res$rows[,1], res$rows[,2], res$cols[,1], res$cols[,2])
legend("topleft", legend=c("First", "Second"), title="Occurrence",
       col=c("blue", "red"), pch=16:17, bg="gray90")
par(op)
@

In \figref{fig:ca-victims-plot} it may be seen that most of the points are
extremely close for the first and second occurrence of a crime,  indicating
that the row profile for a crime is very similar to its corresponding column
profile, with Rape and Pick Pocket as exceptions.

In fact, if the table was symmetric, the row and column points in \figref{fig:ca-victims-plot} would be identcal,
as can be easily demonstrated by analyzing a symmetric version.
<<RVsym, fig.show='hide'>>=
RVsym <- (RepVict + t(RepVict))/2
RVsym.ca <- ca(RVsym)
res <- plot(RVsym.ca)
all.equal(res$rows, res$cols)
@


The first dimension appears to contrast crimes against the person (right) with
crimes against property (left), and it may be that the second dimension
represents degree of violence associated with each crime.
The latter interpretation is consistent with the movement of Rape towards
a higher position and Pickpocket towards a lower one on this dimension.

\end{Example}

\ixoff{correspondence analysis!two-way tables}

\subsection{Corespondence analysis and mosaic displays}
For a two-way table, CA and mosaic displays give complementary views of the 
pattern of association between the row and column variables, but both
are based on the (Pearson) residuals from independence. 
CA shows the row and column categories as points in a 2D (or 3D) space
accounting for the largest proportion of the Pearson $\chisq$, while
mosaics show the association by the pattern of shading in the mosaic tiles.
It is useful to compare them directly to see how associations can be
interpreted from these graphs.

\begin{Example}[TV2]{TV viewing data}
The data on television viewership from \citet{HartiganKleiner:84} was used
as an example of manipulating complex categorical data in \secref{sec:working-complex}
and shown as a three-way mosaic plot in \figref{fig:TV-mosaic}.
From that figure, the main association concerned how viewership across
days of the week varied by TV network, so we first collapse the \data{TV} data
to a $5 \times 3$ two-way table.
<<TV2-1>>=
data("TV", package="vcdExtra")
TV2 <- margin.table(TV, c(1,3))
TV2
@
In this case, the 2D CA solution is exact, meaning that two dimensions account for 
100\% of the association.
<<TV2-ca-results, output.lines=5>>=
TV.ca <- ca(TV2)
TV.ca
@
The plot of this solution is shown in the left panel of \figref{fig:TV-mosaic-ca},
using lines from the origin to the category points for the networks.
<<TV2-ca, eval=FALSE>>=
res <- plot(TV.ca)
segments(0, 0, res$cols[,1], res$cols[,2], col="red", lwd=2)
@

\begin{figure}[!htb]
  \centering
  \includegraphics[width=.49\textwidth]{ch06/fig/TV2-ca}
  \includegraphics[width=.49\textwidth]{ch06/fig/TV2-mosaic}
  \caption{CA plot and mosaic display for the TV viewing data. The days of the week in the mosaic plot were permuted according to their order in the CA solution.}
  \label{fig:TV-mosaic-ca}
\end{figure}
An analogous mosaic display, informed by the CA solution, is
shown in the right panel of \figref{fig:TV-mosaic-ca}.
Here, the days of the week are reordered according to their positions
on the first CA dimension, another example of \IX{effect ordering}.
%uses \code{labeling=labeling\_residuals} to label all residuals in the cells.
<<TV2-mosaic, eval=FALSE>>=
days.order <- order(TV.ca$rowcoord[,1])
mosaic(t(TV2[days.order,]), shade=TRUE, legend=FALSE, 
       labeling=labeling_residuals, suppress=0)
@
In the CA plot, you can see that the dominant dimension separates viewing on Thursday,
with the largest share of viewers watching NBC, from the other weekdays.  
In the mosaic plot, Thursday stands out as the only day with a higher than expected
frequency for NBC, and this is the largest residual in the entire table.
The second dimension in the CA plot separates CBS, with its' greatest proportion of
viewers on Monday, from ABC, with greater viewership on Wednesday and Friday.

\citet[Fig. 2]{Emerson:1998} gives a table listing the shows in each half-hour time
slot.  Could the overall popularity of NBC on Thursday be due to \emph{Friends} or \emph{Seinfeld}?
An answer to this and similar questions requires analysis of the three-way table
(\labref{lab:TV3}) and model-based methods for polytomous outcome variables
described in \secref{sec:genlogit}.


\end{Example}

% \section{Properties of category scores}\label{sec:ca-scores}
% 
% This section illustrates several properties of the \ca
% scores through calculation and visualization.
% 
% \TODO{This section requires a lot of custom programming. Maybe useful,
% but for now, I'm leaving this until later, or just delete this section.}
% 
% \subsection{Optimal category scores}\label{sec:ca-optimal-scores}
% 
% \subsection{Simultaneous linear regressions}\label{sec:ca-linreg}


\section{Multi-way tables: Stacking and other tricks}\label{sec:ca-multiway}

A three- or higher-way table can be analyzed by correspondence
analysis in several ways.
\ixon{correspondence analysis!stacking}
Multiple correspondence analysis (MCA), described in \secref{sec:mca},
is an extension of simple
\ca which analyzes simultaneously all possible two-way tables
contained within a \mway table.
Another approach, described here, is called
\term{stacking} or \term{interactive coding}. This is a bit of a trick,
to force a \mway table into a two-way table for a standard \ca,
but is a useful one.
\ix{interactive coding}
\ix{correspondence analysis!interactive coding}

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{ch06/fig/stacking}
\caption{Stacking approach for a three-way table. Two of the table variables are combined interactively to form the rows of a two-way table.}\label{fig:stacking}
\end{figure}

A
three-way table, of size \(I \times  J \times  K\) can be sliced into
\(I\) two-way tables, each \(J \times  K\).  If the slices are
concatenated vertically, the result is one two-way table, of size \((
I \times  J ) \times  K\), as illustrated in \figref{fig:stacking}.
In effect, the first two variables are
treated as a single composite variable with \(IJ\) levels, which represents the main
effects and interaction between the original variables that were
combined.
\Citet{HeijdenLeeuw:85}
discuss this use of
correspondence analysis for multi-way tables and show how \emph{each} way of
slicing and stacking a contingency table corresponds to the analysis
of a specified \loglin model.
Like the mosaic display, this provides another way to visualize the relations
in a \loglin model.

In particular, for the three-way table with variables $A, B, C$
that is reshaped as a table of
size \(( I \times  J ) \times  K\), the \ca
solution analyzes residuals from the log-linear model
%[AB] [C].
$\LLM{AB,C}$.
That
is, for such a table, the \(I \times  J\) rows represent the joint
combinations of variables A and B.  The expected frequencies under
independence for this table are
\begin{equation}\label{eq:mij-k}
  m_{[ij]k}
%  = \frac{ n_{[ij]+} \,  n_{[++]k} }{n}
  = \frac{ n_{ij+} \,  n_{++k} }{n}
\end{equation}
which are the ML estimates of expected frequencies for the log-linear
model $\LLM{AB, C}$.
The \(\chi^2\) that is decomposed by \ca is the Pearson
\(\chi^2\) for this log-linear model.  When the table is stacked as
\(I \times  ( J \times  K )\) or \(J \times  ( I \times  K )\),
correspondence analysis decomposes the residuals from the log-linear
models $\LLM{A,BC}$ and $\LLM{B,AC}$, respectively, as shown in
\tabref{tab:stacking}.
In this approach, only the associations in separate $[ \, ]$ terms are analysed
and displayed in the \ca maps.
\Citet{HeijdenLeeuw:85}
show how a generalized form of correspondence analysis
can be interpreted as decomposing the difference between two specific
\loglin models, so their approach is more general than is illustrated
here.

\begin{table}[htb]
  \centering
  \caption{Each way of stacking a three-way table corresponds to a loglinear model}\label{tab:stacking}
  \vspace{1ex}
  \begin{tabular}{ll}
   \hline
   \tableheader
   Stacking structure & Loglinear model \\ [.5ex]
   \hline
   \(( I \times  J ) \times  K\) & $\LLM{AB,C}$ \\
   \(I \times  ( J \times  K )\) & $\LLM{A,BC}$ \\
   \(J \times  ( I \times  K )\) & $\LLM{B,AC}$ \\
   \hline
  \end{tabular}
\end{table}

\subsection{Interactive coding in \R}\label{sec:ca-interactiveR}
In the general case of an \nway table, the stacking approach
is similar to that used by \func{ftable} and \func{structable} in \pkg{vcd}
as described in \secref{sec:structable} to flatten \mway tables to a two-way,
printable form, where some variables are assigned to the rows and the
others to the columns. 
Both \func{ftable} and \func{structable} have \func{as.matrix}
methods%
\footnote{This requires at least \R version 3.1.0 or \pkg{vcd} 1.3-2 or later.}
that convert their result into a matrix suitable as input to \func{ca}.

With data in the form of a frequency data frame, you can easily
create interactive coding using \func{interaction} or simply use
\func{paste} to join the levels of stacked variables together.

To illustrate, create a 4-way table of random Poisson counts (with constant mean, $\lambda=15$)
of types of Pet, classified
by Age, Color and Sex.
<<stacking-demo>>=
set.seed(1234)
dim <- c(3, 2, 2, 2)
tab <- array(rpois(prod(dim), 15), dim=dim)
dimnames(tab) <- list(Pet=c("dog","cat","bird"), 
                      Age=c("young","old"), 
                      Color=c("black", "white"), 
                      Sex=c("male", "female"))
@
You can use \func{ftable} to print this, with a formula that assigns \code{Pet} and \code{Age}
to the columns and \code{Color} and \code{Sex} to the rows.
<<stacking-demo1>>=
ftable(Pet + Age ~ Color + Sex, tab)
@
Then, \func{as.matrix} creates a matrix with the levels of the stacked variables combined with some separator character.
Using \code{ca(pet.mat)} would then calculate the CA solution for the stacked table, analyzing only the associations
in the \loglin model \LLM{Pet Age,Color Sex}.%
\footnote{The result would not be at all interesting here. Why?}
<<stacking-demo2, R.options=list(width=96), size='footnotesize'>>=
(pet.mat <- as.matrix(ftable(Pet + Age ~ Color + Sex, tab), sep='.'))
@
With data in a frequency data frame, a
similar result (as a frequency table), can be obtained using \func{interaction} as shown below. The result of
\func{xtabs} looks the same as \code{pet.mat}.
<<stacking-demo3, results='hide'>>=
tab.df <- as.data.frame(as.table(tab))
tab.df <- within(tab.df, 
  {Pet.Age = interaction(Pet,Age)
  Color.Sex = interaction(Color,Sex)
  })               
xtabs(Freq ~ Color.Sex + Pet.Age, data=tab.df)
@

% However, those functions do not create a suitable
% matrix or table that can be used as input to \func{ca}.
% \DONE{The rest here depends on writing general functions for this: \func{as.matrix.ftable}
% (now in \pkg{stats}) and \func{as.matrix.structable} (now in vcd).
% }

\begin{Example}[suicide1]{Suicide rates in Germany}

To illustrate the use of correspondence analysis for the analysis for
three-way tables, we use data on suicide rates in West Germany
classified by sex, age, and method of suicide used.  The data, from
\citet[Table 1]{Heuer:79}
have been discussed by
\citet{Friendly:91,Friendly:94a,HeijdenLeeuw:85}
and others.

The original \(2
\times  17 \times  9\) table contains 17 age groups from 10 to 90 in
5-year steps and 9 categories of suicide method, contained in the
frequency data frame \data{Suicide} in \pkg{vcd}, with
table variables \var{sex}, \var{age} and \var{method}.
To avoid extremely
small cell counts and cluttered displays,
this example uses a reduced table in which age
groups are combined in the variable \var{age.group}, a factor
with 15 year intervals except for the last
interval, which includes ages 70--90;
the methods ``toxic gas'' and
``cooking gas'' were collapsed (in the variable \var{method2})
% and methods ``knife'' and ``other'' were deleted,
giving the \(2 \times  5 \times  8\) table shown
%in \tabref{tab:suidat}.
in the output below.
These changes do not affect the general
nature of the data or conclusions drawn from them.

In this example, we decided to stack the combinations of
age and sex, giving an analysis of the \loglin model
$\LLM{AgeSex, Method}$, to show how the age-sex categories
relate to method of suicide.

In the case of a frequency data frame, it is quite simple to
join two or more factors to form the rows of a new two-way table.
Here we use \func{paste} to form a new, composite
factor, called \code{age\_sex} here, abbreviating \code{sex} for display purposes.

<<ca-suicide1>>=
data("Suicide", package="vcd")
# interactive coding of sex and age.group
Suicide <- within(Suicide, {
  age_sex <- paste(age.group, toupper(substr(sex,1,1)))
  })
@
Then, use \func{xtabs} to construct the two-way table \code{suicide.tab}:
<<ca-suicide2>>=
suicide.tab <- xtabs(Freq ~ age_sex + method2, data=Suicide)
suicide.tab
@
The results of the \ca of this table are shown below:
<<ca-suicide3, output.lines=13>>=
suicide.ca <- ca(suicide.tab)
summary(suicide.ca)
@
It can be seen that 92.6\% of the $\chi^2$ for this model is accounted for in
the first two dimensions.  Plotting these gives the display shown in
\figref{fig:ca-suicide-plot}.

<<ca-suicide-plot, echo=2, h=6, w=6, out.width='.7\\textwidth', cap='2D CA solution for the stacked [AgeSex][Method] table of the suicide data', scap='2D CA solution for the stacked AgeSex, Method table of the suicide data'>>=
op <- par(cex=1.3, mar=c(4,4,1,1)+.1)
plot(suicide.ca)
par(op)
@

Dimension 1 in the plot separates males (right) and females (left),
indicating a large difference between suicide profiles of males and
females with respect to methods of suicide.
The second dimension is mostly ordered by age with younger
groups at the bottom and older groups at the top.  Note also that the
positions of the age groups are roughly parallel for the two
sexes.
Such a pattern indicates that sex and age do not interact in this analysis.

The relation between the age--sex groups and methods
of suicide can be approximately interpreted in terms of similar distance and
direction from the origin, which represents the marginal row and
column profiles.  Young males are more likely to commit suicide by
gas or a gun, older males by hanging, while young females are more
likely to ingest some toxic agent and older females by jumping or
drowning.
\end{Example}

\begin{Example}[suicide2]{Suicide rates in Germany}[mosaic plot]
For comparison, it is useful to see how to construct a mosaic display
showing the same associations for the \loglin model $\LLM{AS,M}$
as in the \ca plot.  To do this, we first construct the
three-way table, \code{suicide.tab3},
<<ca-suicide4>>=
suicide.tab3 <- xtabs(Freq ~ sex + age.group + method2, data=Suicide)
@
As discussed in \chref{ch:mosaic}, mosaic plots are sensitive both to
the order of variables used in successive splits, and to the order of
levels within variables and are most effective when these orders
are chosen to reflect the some meaningful ordering.

In the present example, \code{method2} is an unordered table factor,
but \figref{fig:ca-suicide-plot} shows that the methods of
suicide vary systematically with both sex and age, corresponding
to dimensions 1 and 2 respectively.  Here we choose to reorder the
table according to the coordinates on Dimension 1. We also delete
the low-frequency \code{"other"} category to simplify the display.
<<ca-suicide5>>=
# methods, ordered as in the table
suicide.ca$colnames
# order of methods on CA scores for Dim 1
suicide.ca$colnames[order(suicide.ca$colcoord[,1])]
# reorder methods by CA scores on Dim 1
suicide.tab3 <- suicide.tab3[, , order(suicide.ca$colcoord[,1])]
# delete "other"
suicide.tab3 <- suicide.tab3[,, -5]
ftable(suicide.tab3)
@

To construct the mosaic display for the same model analysed by \ca, we use the
argument \verb|expected=~age.group*sex + method2| to supply the model formula.
For this large table, it is useful to tweak the labels for the \code{method2}
variable to reduce overplotting; the \code{labeling\_args} argument provides
many options for customizing \code{strucplot} displays.
<<ca-suicide-mosaic, h=6, w=6, out.width='.7\\textwidth', cap='Mosaic display showing deviations from the model [AgeSex][Method] for the suicide data', scap='Mosaic display showing deviations from the model AgeSex Method for the suicide data', fig.pos='!htb'>>=
library(vcdExtra)
mosaic(suicide.tab3, shade=TRUE, legend=FALSE,
       expected=~age.group*sex + method2,
       labeling_args=list(abbreviate_labs=c(FALSE, FALSE, 5)),
                          rot_labels = c(0, 0, 0, 90))
@
This figure (\figref{fig:ca-suicide-mosaic})
again shows the prevalence of \code{gun} and
\code{gas} among younger males and decreasing with age, whereas use of \code{hang}
increases with age.
For females, these three methods are used less
frequently, whereas \code{poison}, \code{jump}, and \code{drown} occur more often.
You can also see that for females the excess prevalence of these high frequency
methods varies somewhat
less with age than it does for males.

\end{Example}
\ixoff{correspondence analysis!stacking}

\subsection{Marginal tables and supplementary variables}\label{ca:marginal}

\ix{correspondence analysis!supplementary variables}
An \nway\ table in frequency form or case form is automatically collapsed
over factors which are not listed in the
call to \func{xtabs} when creating the table input for \func{ca}.
The analysis gives a \term{marginal model} for the categorical variables which
\emph{are} listed.

The positions of the categories of the omitted variables
may nevertheless be recovered, by treating them as \term{supplementary variables},
given as additional rows or columns in the two-way table.
A supplementary variable is ignored in finding the CA solution,
but its categories are then projected into that space.
This is another useful trick to extend traditional CA to higher-way tables.

To illustrate, the code below list only the \code{age}
and \code{method2} variables, and hence produces an analysis
collapsed over \code{sex}.
This ignores not only the effect of sex itself,
but also all associations of age and method with sex,
which are substantial. We don't show the \func{ca} result
or the plot yet.

<<ca-suicide6>>=
# two way, ignoring sex
suicide.tab2 <- xtabs(Freq ~ age.group + method2, data=Suicide)
suicide.tab2
suicide.ca2 <- ca(suicide.tab2)
@

To treat the levels of \code{sex} as supplementary points, we calculate
the two-way table of sex and method, and append this to the
\code{suicide.tab2} as additional rows:

<<ca-suicide7>>=
# relation of sex and method
suicide.sup <- xtabs(Freq ~ sex + method2, data=Suicide)
suicide.tab2s <- rbind(suicide.tab2, suicide.sup)
@
In the call to \func{ca}, we then indicate these last two rows
as supplementary:
<<ca-suicide8, output.lines=11>>=
suicide.ca2s <- ca(suicide.tab2s, suprow=6:7)
summary(suicide.ca2s)
@
This CA analysis has the same total Pearson chi-square,
$\chi^2 (28) = 3422.5$ as the result of \code{chisq.test(suicide.tab2)}.
However, the scree plot display above shows that the association between age and method
is essentially one-dimensional,
but note also that dimension 1 (``age-method'') in this analysis has nearly the same inertia (0.0604)
as the second dimension (0.0596) in the analysis of the stacked table.
We plot the CA results as shown below
(see \figref{fig:ca-suicide-sup}), and
add a line connecting the supplementary points for sex.
<<ca-suicide-sup, h=3, w=6, out.width='.9\\textwidth', cap='2D CA solution for the [Age] [Method] marginal table. Category points for Sex are shown as supplementary points', scap='2D CA solution for the Age Method marginal table'>>=
op <- par(cex=1.3, mar=c(4,4,1,1)+.1)
res <- plot(suicide.ca2s, pch=c(16, 15, 17, 24))
lines(res$rows[6:7,])
par(op)
@
Comparing this graph with \figref{fig:ca-suicide-plot},
you can see that ignoring sex has collapsed the differences between
males and females which were the dominant feature of the analysis
including sex.  The dominant feature in \figref{fig:ca-suicide-sup}
is the Dimension 1 ordering of both age and method.
However, as in \figref{fig:ca-suicide-plot}, the supplementary points for sex
point toward the methods that are more prevalent for females
and males.


\section{Multiple correspondence analysis}\label{sec:mca}

Multiple \ca (MCA) is designed to display the relationships of the categories
of two or more discrete variables, but it is best used for \mway tables
where the extensions of classical CA described in \secref{sec:ca-multiway}
do not suffice.
Again, this is motivated by the desire to provide
an \emph{optimal scaling} of categorical variables, giving scores for the
discrete variables in an \nway table with desirable properties and which
can be plotted to visualize the relations among the category points.

%There are several complementary ways of defining MCA as an optimal scaling of categorical data.
The most typical development of MCA
starts by defining indicator (``dummy'') variables
for each category and reexpresses the \nway \ctab in the form
of a cases by variables indicator matrix, $\mat{Z}$.
Simple \ca for a two-way table can, in fact, be derived as the
canonical correlation analysis of the indicator matrix.

Unfortunately, the generalization to more than two variables follows
a somewhat different path, so that simple CA does not turn out to be
precisely a special case of MCA in some respects, particularly in the
decomposition of an interpretable \chisq over the dimensions in
the visual representation.

Nevertheless, MCA does provide a useful graphic portrayal of the
\emph{bivariate} relations among any number of categorical variables,
and has close relations to the mosaic matrix (\secref{sec:mosmat}).
If its limitations are understood, it is helpful in
understanding large, multivariate categorical data sets,
in a similar way to the use of scatterplot matrices
and dimension-reduction techniques
(e.g., \IX{principal component analysis}) for quantitative data.

\ix{scatterplot matrix}
\ix{mosaic matrix}

%\TODO{I've run into a wall here. The \pkg{ca} functions
%\func{mjca} and \func{plot.mjca} are too limited for the
%plots I want to do here.
%}

\subsection{Bivariate MCA}\label{sec:mca-bi}
\ixon{multiple correspondence analysis!bivariate}

For the hair color, eye color data, the \IX{indicator matrix} $\mat{Z}$
has 592 rows and $4+4=8$ columns.  The columns refer to the eight
categories of hair color and eye color and the rows to the
592 students in Snee's \citeyear{Snee:74} sample.

For simplicity, we show the calculation of the indicator matrix
below in frequency form, using \func{model.matrix} to compute the
dummy (0/1) variables
for the levels of hair color (\code{Hair1}--\code{Hair4})
and eye color (\code{Eye1}--\code{Eye4}).

<<mca-indicator1, size='footnotesize'>>=
haireye.df <- cbind(
    as.data.frame(haireye),
    model.matrix(Freq ~ Hair + Eye, data=haireye,
        contrasts.arg=list(Hair=diag(4), Eye=diag(4)))[,-1]
    )
haireye.df
@

Thus, the first row in \code{haireye.df} represents the 68 individuals
having black hair (\code{Hair1=1}) and brown eyes (\code{Eye1=1}).
The indicator matrix $\mat{Z}$ is then computed by replicating the
rows in \code{haireye.df} according to the \code{Freq} value,
using the function \code{expand.dft}.  The result has 592 rows
and 8 columns.

<<mca-indicator2>>=
Z <- expand.dft(haireye.df)[,-(1:2)]
vnames <- c(levels(haireye.df$Hair), levels(haireye.df$Eye))
colnames(Z) <- vnames
dim(Z)
@
Note that if the indicator matrix is partitioned as
$\mat{Z} = [ \mat{Z}_1 , \mat{Z}_2 ]$, corresponding to the two sets of
categories, then the contingency table is given by
$\mat{N} = \mat{Z}_1 \trans \mat{Z}_2$.
<<mca-indicator3>>=
(N <- t(as.matrix(Z[,1:4])) %*% as.matrix(Z[,5:8]))
@
With this setup, MCA can be described as the application of the simple \ca
algorithm to the indicator matrix $\mat{Z}$.
This analysis would yield scores for the rows of $\mat{Z}$ (the cases),
usually not of direct interest
and for the columns (the categories of both variables).
As in simple CA, each row point is the weighted average of the scores
for the column categories, and each column point is the weighted average
of the scores for the row observations.%
\footnote{Note that, in principle, this use of an indicator matrix could be
extended to three (or more) variables.  That extension is more easily
described using an equivalent form, the \term{Burt matrix}, described
in \secref{sec:mca-burt}.
}

Consequently, the point for any category is the centroid of all the
observations with a response in that category, and
all observations with the same response pattern coincide.
As well, the origin reflects the weighted average of the categories for
\emph{each} variable.  As a result, category points with low marginal
frequencies will be located further away from the origin,
while categories with high marginal frequencies will be closer to the
origin.
For a binary variable, the two category points will appear on a line
through the origin, with distances inversely proportional to their
marginal frequencies.

\begin{Example}[haireye4]{Hair color and eye color}
For expository purposes,
we illustrate the analysis of the indicator matrix below for the hair color,
eye color data using \func{ca}, rather than the function \func{mjca}
which is designed for a more general approach to MCA.

<<mca-haireye0, fig.keep='none'>>=
Z.ca <- ca(Z)
res <- plot(Z.ca, what=c("none", "all"))
@
In the call to \code{plot.ca}, the argument \code{what}
is used to suppress the display of the row points for the cases.
The plot shown in \figref{fig:mca-haireye1} is an enhanced
version of this basic plot.

<<mca-haireye1, echo=FALSE, h=6, w=8, out.width='.8\\textwidth', cap='Correspondence analysis of the indicator matrix Z for the hair color, eye color data. The category points are joined separately by lines for the hair color and eye color categories.'>>=
# customized plot
res <- plot(Z.ca, what=c("none", "all"), labels=0, pch='.', xpd=TRUE)

# extract factor names and levels
coords <- data.frame(res$cols)
coords$factor <- rep(c("Hair", "Eye"), each=4)
coords$levels <- rownames(res$cols)
coords
# sort by Dim 1
coords <- coords[ order(coords[,"factor"], coords[,"Dim1"]), ]

cols <- c("blue", "red")
nlev <- c(4,4)
text(coords[,1:2], coords$levels, col=rep(cols, nlev), pos=2, cex=1.2)
points(coords[,1:2], pch=rep(16:17, nlev), col=rep(cols, nlev), cex=1.2)

lines(Dim2 ~ Dim1, data=coords, subset=factor=="Eye",  lty=1, lwd=2, col=cols[1])
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Hair", lty=1, lwd=2, col=cols[2])
@

Comparing \figref{fig:mca-haireye1}
with \figref{fig:ca-haireye-plot}, we see that the general pattern of
the hair color and eye color categories is the same in the analysis of
the contingency table (\figref{fig:ca-haireye-plot}) and the analysis of the
indicator matrix (\figref{fig:mca-haireye1}), except that the axes are scaled
differently---the display has been stretched along the second (vertical)
dimension.
The interpretation is the same: Dimension 1 reflects a dark--light ordering
of both hair and eye colors, and Dimension 2 reflects something that largely
distinguishes red hair and green eyes from the other categories.

Indeed, it can be shown \citep{Greenacre:84,Greenacre:2007}
that the two displays are identical, except for changes in scales along
the axes.
There is no difference at all between the displays in standard coordinates.
\citet[pp. 130--134]{Greenacre:84} describes the precise relations
between the geometries of the two analyses.

\end{Example}
\ixoff{multiple correspondence analysis!bivariate}

Aside from the largely cosmetic difference in relative scaling of the axes,
a major difference between analysis of the \ctab\ and analysis of the
indicator matrix is in the decomposition of principal inertia and corresponding
$\chisq$ contributions
for the dimensions. The plot axes in \figref{fig:mca-haireye1}
indicate 24.3\% and 19.2\% for the contributions of the two dimensions,
whereas \figref{fig:ca-haireye-plot} shows 89.4\% and 9.5\%.
This difference is the basis for the more general development of MCA methods
and is reflected in the \func{mcja} function illustrated later in this chapter.
But first, we describe a second approach to extending simple CA to the
multivariate case based on the \term{Burt matrix}.

\subsection{The Burt matrix}\label{sec:mca-burt}
\ixon{Burt matrix}

The same solution for the category points as in the
analysis of the indicator matrix may be obtained more simply
from the so-called \term{Burt matrix} \citep{Burt:50},
\begin{equation*}%\label{eq:burt2}
 \mat{B} = \mat{Z}\trans \mat{Z}
 =
 \left[
 \begin{array}{ll}
 \mat{N}_{1} & \mat{N} \\
 \mat{N}\trans & \mat{N}_{2} \\
 \end{array}
 \right]
 \comma
\end{equation*}
where $\mat{N}_{1}$ and $\mat{N}_{2}$ are diagonal matrices containing
the marginal frequencies of the two variables (the column sums of
$\mat{Z}_1$ and $\mat{Z}_2$).  In this representation, the contingency
table of the two variables, $\mat{N}$ appears in the off-diagonal block,
$\mat{N}$ in this equation. This calculation is shown below.

<<Burt1>>=
Burt <- t(as.matrix(Z)) %*% as.matrix(Z)
rownames(Burt) <- colnames(Burt) <- vnames
Burt
@

The standard coordinates from an analysis of the Burt matrix
$\mat{B}$ are identical to those of $\mat{Z}$.
(However, the singular values of $\mat{B}$ are the squares of those of $\mat{Z}$.)
Then, the following code,
using \code{Burt} produces the same display of the category points for
hair color and eye color as shown for the indicator matrix \code{Z} in \figref{fig:mca-haireye1}.

<<Burt2, fig.keep='none'>>=
Burt.ca <- ca(Burt)
plot(Burt.ca)
@

\ixoff{Burt matrix}

\subsection{Multivariate MCA}\label{sec:mca-multi}
The coding of categorical variables in an indicator matrix
and the relationship to the Burt matrix
provides
a direct and natural way to extend this analysis to more than two variables.
If there are $Q$ categorical variables, and variable $q$ has $J_q$
categories, then the $Q$-way \ctab, of size
$J = \prod_{q=1}^Q J_q = J_1 \times J_2 \times \cdots \times J_Q$,
with a total of $n = n_{++\cdots}$ observations
may be represented by the partitioned $(n \times J)$ indicator matrix
$[ \mat{Z}_1 \, \mat{Z}_2  \, \dots \, \mat{Z}_Q ]$.


Then the Burt matrix is the symmetric partitioned matrix
\begin{equation}\label{eq:burt}
 \mat{B} = \mat{Z}\trans \mat{Z}
 =
 \left[
 \begin{array}{llll}
 \mat{N}_{1} & \mat{N}_{12} & \cdots & \mat{N}_{1Q}\\
 \mat{N}_{21} & \mat{N}_{2} & \cdots & \mat{N}_{2Q}\\
 \vdots        & \vdots         & \ddots  & \vdots       \\
 \mat{N}_{Q1} & \mat{N}_{Q2} & \cdots & \mat{N}_{Q}\\
 \end{array}
 \right]
 \comma
\end{equation}
where again the diagonal blocks $\mat{N}_{i}$ contain the one-way
marginal frequencies. The off-diagonal blocks $\mat{N}_{ij}$
contain the bivariate marginal contingency tables for each pair
$(i,j)$ of variables.

Classical MCA (see, e.g., \cite{Greenacre:84,GowerHand:96})
can then be defined as a singular value decomposition of the matrix $\mat{B}$ which produces scores for the
categories of \emph{all} variables so that the greatest proportion of the
bivariate, pairwise associations in all blocks (including the diagonal blocks) is accounted for in
a small number of dimensions.

In this respect, MCA resembles multivariate methods for quantitative
data based on the joint bivariate correlation or covariance matrix
($\mat{\Sigma}$)
and there is some justification to regard the Burt matrix as the
categorical analog of $\mat{\Sigma}$.%
\footnote{For multivariate normal data, however, the mean vector and
covariance matrix are sufficient statistics, so all higher-way relations
are captured in the covariance matrix.  This is not true of the Burt
matrix.
Moreover, the covariance matrix is typically expressed in terms of mean-centered
variables, while the Burt matrix involves the marginal frequencies.  A more accurate
statement is that the uncentered covariance matrix is analogous to the Burt matrix.}

There is a close connection between this analysis and the bivariate mosaic
matrix (\secref{sec:mosmat}):
The mosaic matrix displays the residuals from independence for each
pair of variables, and thus provides a visual representation of the Burt matrix.
The one-way margins shown (by default) in the diagonal cells
reflect the diagonal matrices $\mat{N}_{i}$ in \eqref{eq:burt}.
The total amount of shading in all the individual mosaics
portrays the total pairwise associations decomposed by MCA.
See \citet{Friendly:99b} for further details.

For  interpretation of MCA plots, we note the following relations
\citep[\S 5.2]{Greenacre:84}:%
\footnote{This book, now out of print, is available for free download at \url{http://www.carme-n.org/}}
\begin{itemize*}
\item The inertia contributed by a given variable increases with the
number of response categories.
\item The centroid of the categories for each discrete variable
is at the origin of the display.
\item For a particular variable,
the inertia contributed by a given category increases as the marginal
frequency in that category \emph{decreases}.
Low frequency points therefore appear further from the origin.
\item The category points for a binary variable lie on a line
through the origin.  The distance of each point to the origin is
inversely related to the marginal frequency.
\end{itemize*}

\begin{Example}[marital3]{Marital status and pre- and extramarital sex}
The data on the relation between marital status and reported
premarital and extramarital sex was explored earlier using mosaic
displays in \exref{ex:marital1} and \exref{ex:marital2}.

Using the \Rpackage{ca}, an MCA analysis of the \data{PreSex} data is carried out
using \func{mjca}.
This function typically takes a data frame in \emph{case form}
containing the factor variables, but converts a table to this form.
%  so we first convert the 4-way
%  \data{PreSex} table to a frequency data frame, and then expand it
%  to case for using \func{expand.dft}.
% 
% <<presex-mca1>>=
% data("PreSex", package="vcd")
% PreSex <- aperm(PreSex, 4:1)   # order variables G, P, E, M
% presex.df <- expand.dft(as.data.frame(PreSex))
% @
% 
This example analyzes the Burt matrix calculated from the \code{PreSex}
data, specified as \code{lambda="Burt"}
<<presex-mca2, output.lines=10>>=
data("PreSex", package="vcd")
PreSex <- aperm(PreSex, 4:1)   # order variables G, P, E, M
presex.mca <- mjca(PreSex, lambda="Burt")
summary(presex.mca)
@
The output from \func{summary} seems to show that 77.6\% of the total inertia
is accounted for in two dimensions. A basic, default plot of the MCA solution
is provided by the \func{plot} method for \class{mjca} objects.
<<presex-mca3, fig.keep='none', results='hide'>>=
plot(presex.mca)
@
This plotting method is not very flexible in terms of control of graphical parameters
or the ability to add additional annotations (labels, lines, legend) to ease
interpretation.  Instead, we use the plot method to create an empty plot
(with no points or labels), and return the calculated plot coordinates (\code{res})
for the categories. A bit of processing of the coordinates provides the customized
display shown in \figref{fig:presex-mca-plot}.

<<presex-mca-plot, h=6, w=6, out.width='.8\\textwidth', cap='MCA plot of the Burt matrix for the PreSex data. The category points are joined separately by lines for the factor variables.', fig.pos='!htb', out.extra='trim=0 10 20 60', size='footnotesize'>>=
# plot, but don't use point labels or points
res <- plot(presex.mca, labels=0, pch='.', cex.lab=1.2)

# extract factor names and levels
coords <- data.frame(res$cols, presex.mca$factors)
nlev <- presex.mca$levels.n
fact <- unique(as.character(coords$factor))

cols <- c("blue", "red", "brown", "black")
points(coords[,1:2], pch=rep(16:19, nlev), col=rep(cols, nlev), cex=1.2)
text(coords[,1:2], label=coords$level, col=rep(cols, nlev), pos=3, 
     cex=1.2, xpd=TRUE)
lwd <- c(2, 2, 2, 4)
for(i in seq_along(fact)) {
  lines(Dim2 ~ Dim1, data = coords, subset = factor==fact[i], 
        lwd = lwd[i], col = cols[i])
}

legend("bottomright", legend = c("Gender", "PreSex", "ExtraSex", "Marital"),
  title = "Factor", title.col = "black",
  col = cols, text.col = cols, pch = 16:19,
  bg = "gray95", cex = 1.2)
@
As indicated above, the category points for each factor appear on
lines through the origin, with distances inversely proportional to their
marginal frequencies. For example, the categories for No premarital and extramarital
sex are much larger than the corresponding Yes categories, so the former are
positioned closer to the origin.  In contrast, the categories of gender and
marital status are more nearly equal marginally.

Another aspect of interpretation of \figref{fig:presex-mca-plot} concerns the alignment
of the lines for different factors.
The positions of the category points on Dimension 1 suggest that Women
are less likely to have had pre-marital and extra-marital sex
and that still being married is associated with the absence of pre- and extra-marital sex.
As well, the lines for gender and marital status are nearly at right
angles, suggesting that these variables are unassociated.
This interpretation is more or less correct, but it is only approximate in this
MCA scaling of the coordinate axes.  An alternative scaling, based on a
\term{biplot} representation is described in \secref{sec:biplot}.

If you compare the MCA result in \figref{fig:presex-mca-plot} with the
mosaic matrix in \figref{fig:marital-pairs}, you will see that they
are both showing the bivariate pairwise associations among these variables,
but in different ways. The mosaic plots show the details of marginal
and joint frequencies together with residuals from independence for
each $2 \times 2$ marginal subtable.  The MCA plot using the Burt matrix
summarizes each category point in terms of a 2D representation of
contributions to total inertia (association).
\end{Example}

\subsubsection{Inertia decomposition}

The transition from simple CA to MCA is straight-forward in terms of
the category scores derived from the indicator matrix $\mat{Z}$ or
the Burt matrix, $\mat{B}$.  It is less so in terms of the
calculation of total inertia,
and therefore in the chi-square values and corresponding percentages
of association accounted for in some number of dimensions.

In simple CA, the total inertia is $\chisq /n$, and it therefore
makes sense to talk of percentage of association accounted for
by each dimension.
But in MCA of the indicator matrix
the total inertia, $\sum \lambda^2$,
is simply $(J - Q)/Q$, because the inertia
of each subtable, $\mat{Z}_i$ is equal to its dimensionality,
$J_i - 1$, and the total inertia of an indicator matrix is
the average of the inertias of its subtables.
Consequently, the average inertia per dimension is $1/Q$,
and it is common to interpret only those dimensions that
exceed this average (analogous to the use of 1 as a threshold for
eigenvalues in principal components analysis).

To more adequately reflect the percentage of association in MCA, 
\citet{Greenacre:90},
%(see also \citet[\C 19]{Greenacre:2007} for details), 
revising an earlier proposal by
\citet{Benzecri:77}, 
suggested the calculation of
\term{adjusted inertia}, which ignores the contributions of
the diagonal blocks in the Burt matrix,
\begin{equation}\label{eq:benzecri}
(\lambda_i^{\star})^2 =
{\left[ \frac{Q}{Q-1} ( \lambda_i^Z - \frac{1}{Q} ) \right]}^2
\end{equation}
as the principal inertia due to the dimensions with $(\lambda^Z)^2 > 1/Q$.
This adjustment
expresses the contribution of each dimension as
$ (\lambda_i^{\star})^2 / \sum (\lambda_i^{\star})^2$,
with the summation over only dimensions with $(\lambda^Z)^2 > 1/Q$.

A related method, also handled by \func{mjca},
is \term{joint correspondence analysis}
%\citep{Greenacre:88,Greenacre:1994},
(\citealp{Greenacre:1994}, \citealp[\C 19]{Greenacre:2007})
an iterative method that replaces the diagonal blocks of the
Burt matrix with values that minimize their impact on inertia.
Unlike MCA, solutions in JCA are not nested, however.


\begin{Example}[titanic2]{Survival on the \emph{Titanic}}

An MCA analysis of the \data{Titanic} data is carried out
using \func{mjca}
as shown below.  
%As in \exref{ex:marital3}, this needs to be converted to a data frame in case form.

<<titanic-mca1>>=
titanic.mca <- mjca(Titanic)
@
\func{mjca} allows different scaling methods for the contributions to inertia
of the different dimensions.  The default (\code{lambda="adjusted"}), used here, is the adjusted inertias
as in \eqref{eq:benzecri}.
<<titanic-mca2, output.lines=9>>=
summary(titanic.mca)
@

Using similar code to that used in \exref{ex:marital3},
\figref{fig:titanic-mca-plot} shows an enhanced version of the default plot that connects
the category points for each factor by lines using the result returned by
the \func{plot} function.
<<titanic-mca3, fig.keep='none', results='hide', echo=FALSE>>=
res <- plot(titanic.mca)
@

<<titanic-mca-plot, echo=FALSE, h=6, w=6, out.width='.8\\textwidth', cap='MCA plot of the Titanic data. The category points are joined separately by lines for the factor variables.', fig.pos='!htb'>>=
# plot, but don't use point labels or points
res <- plot(titanic.mca, labels=0, pch='.', cex.lab=1.2)

# extract factor names and levels
coords <- data.frame(res$cols, titanic.mca$factors)

cols <- c("blue", "red", "brown", "black")
nlev <- c(4,2,2,2)
points(coords[,1:2], pch=rep(16:19, nlev), col=rep(cols, nlev), cex=1.2)
pos <- c(3,1,1,3)
text(coords[,1:2], labels=coords$level, col=rep(cols, nlev), pos=rep(pos,nlev), cex=1.1, xpd=TRUE)

coords <- coords[ order(coords[,"factor"], coords[,"Dim1"]), ]

lines(Dim2 ~ Dim1, data=coords, subset=factor=="Class", lty=1, lwd=2, col="blue")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Sex",  lty=1, lwd=2, col="red")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Age",  lty=1, lwd=2, col="brown")
lines(Dim2 ~ Dim1, data=coords, subset=factor=="Survived",  lty=1, lwd=2, col="black")

legend("topleft", legend=c("Class", "Sex", "Age", "Survived"),
  title="Factor", title.col="black",
	col=cols, text.col=cols, pch=16:19,
	bg="gray95", cex=1.2)
@

In this plot, the points for each factor have the property that the sum of coordinates
on each dimension, weighted inversely by the marginal proportions, equals
zero. Thus
high frequency categories (e.g., Adult and Male) are close to the origin.

The first dimension is perfectly aligned with gender, and also
strongly aligned with Survival.  The second dimension pertains mainly to
Class and Age effects.  Considering those points which differ from the
origin most similarly (in distance and direction) to the point for Survived,
gives the interpretation that survival was associated with being female
or upper class or (to a lesser degree) being a child.


\end{Example}

% \section{Extended MCA: Showing interactions in $2^Q$ tables}\label{sec:ca-mcainter}
% \TODO{Probably skip this section}

\section{Biplots for contingency tables}\label{sec:biplot}

\ixon{biplot}
Like \ca, the \term{biplot}
\citep{BraduGabriel:78,Gabriel:71,Gabriel:80,Gabriel:81,Gower-etal:2011}
is a visualization method
which uses the SVD to display a matrix in a low-dimensional
(usually 2-dimensional) space.
They differ in the relationships in the data that are portrayed,
however:
\begin{itemize}
  \item In \ca the (weighted, \chisq) \emph{distances} between row points and
  distances between column points are designed to reflect \emph{differences} between the row profiles
  and column profiles.

  \item In the biplot, on the other hand,
  row and column points are represented by \emph{vectors} from the origin
  such that the projection
  (inner product) of the vector $\vec{a}_i$ for row $i$
  on $\vec{b}_j$ for column $j$ approximates the data element
  $y_{ij}$,
\begin{equation}\label{eq:biplot1}
 \mat{Y} \approx \mat{A} \mat{B}\trans \Longleftrightarrow
 y_{ij} \approx \vec{a}_i \trans \vec{b}_j
 \period
\end{equation}
\end{itemize}
Geometrically, \eqref{eq:biplot1} may be described as approximating the data value $y_{ij}$
by the projection of the end point of vector $\vec{a}_i$
on $\vec{b}_j$ (and vice-versa), as shown in \figref{fig:Scalarproduct}.

\begin{figure}[!htb]
 \centering
 \includegraphics[width=0.5\textwidth]{ch06/fig/Scalarproduct}
 \caption[The scalar product of vectors of two points from the origin]{The scalar product of vectors of two points from the origin is the
 length of the projection of one vector on the other.}\label{fig:Scalarproduct}
\end{figure}

\subsection{CA bilinear biplots}
As in CA, there are a number of different representations of coordinates for row and column
points for a \ctab
within a biplot framework.  One set of
connections between CA and the biplot can be
be seen through the \emph{reconstitution formula}, giving the decomposition of the
correspondence matrix $\mat{P} = \mat{N} / n$ in terms of the standard coordinates
$\mat{\Phi}$ and $\mat{\Gamma}$ defined in \eqref{eq:scoord1} and \eqref{eq:scoord2}
as:

\begin{equation}\label{eq:reconstitution1}
  p_{ij} = r_i c_j \left(1 + \sum_{m=1}^M \sqrt{\lambda_m} \phi_{im} \gamma_{jm} \right)
\end{equation}
or, in matrix terms,
\begin{equation}\label{eq:reconstitution2}
  \mat{P} = \mat{D}_r (\vec{1} \vec{1}\trans + \mat{\Phi} \mat{D}_\lambda^{1/2} \Gamma\trans) \mat{D}_c
\end{equation}
The CA solution approximates this by a sum over $d \ll M  $ dimensions, or by using only
the first $d$ (usually 2) columns of $\mat{\Phi}$ and $\mat{\Gamma}$.

\eqref{eq:reconstitution1} can be re-written in biplot scalar form as
\begin{equation}\label{eq:rowprincipal}
  \left( \frac{p_{ij}} {r_i c_j} \right) - 1
  \approx
  \sum_{m=1}^d (\sqrt{\lambda_m} \phi_{im})  \gamma_{jm}
=  \sum_{m=1}^d f_{im} \gamma_{jm}
\end{equation}
where $f_{im} = (\sqrt{\lambda_m} \phi_{im})$ gives the principal coordinates of the row
points.  The left-hand side of \eqref{eq:rowprincipal} contains the
\term{contingency ratios}, $p_{ij} /{r_i c_j}$ of the observed cell
probabilities to their expected values under independence.
This shows that an \term{asymmetric CA plot} of row principal coordinates $\mat{F}$
and the column standard coordinates $\mat{\Gamma}$ is a biplot that approximates
the deviations of the contingency ratios from their values under independence.

In the \Rpackage{ca}, this plot is obtained by specifying \code{map="rowprincipal"} in the call to
\func{plot}, or \code{map="colprincipal"} to plot the column points in principal coordinates.
It is typical in such biplots to display one set of coordinates as points
and the other as vectors from the origin, as controlled by the \code{arrows} argument,
so that one can interpret the data values represented as approximated by
the projections of the points on the vectors.

Two other types asymmetric ``maps'' are also defined with different scalings
that turn out to have better visual properties in terms of representing the relations
between the row and column categories, particularly when the strength of association
(inertia) in the data is low.
\begin{itemize*}
  \item The option \code{map="rowgab"} (or \code{map="colgab"}) gives a biplot form proposed by
  \citet{GabrielOdoroff:1990} with the rows (columns) shown in principal coordinates and the
  columns (rows) in standard coordinates multiplied by the mass $c_j$ ($r_i$) of the corresponding
  point.
  \item The \emph{contribution biplot} for CA \citep{Greenacre:2013}, with the option
  \code{map="rowgreen"} (or \code{map="colgreen"}) provides a reconstruction of the standardized
  residuals from independence, using the points in standard coordinates multiplied by the square
  root of the corresponding masses.  This has the nice visual property of showing more directly
  the contributions of the vectors to the low-dimensional solution.
  %\TODO{Check this description!}
\end{itemize*}


%\DONE{Need an example here}
\begin{Example}[suicide3]{Suicide rates in Germany}[biplot]
To illustrate the biplot representation, we continue with the
data on suicide rates in Germany from \exref{ex:suicide1}
using the stacked table \code{suicide.tab} comprised of
the age--sex combinations as rows and methods of suicide as columns.

<<suicide-ca>>=
suicide.tab <- xtabs(Freq ~ age_sex + method2, data=Suicide)
suicide.ca <- ca(suicide.tab)
@
Using this result, \code{suicide.ca},
in the call to \func{plot} below, we use \code{map="colgreen"} and vectors
represent the methods of suicide, as shown in \figref{fig:ca-suicide-biplot}.
<<ca-suicide-biplot, echo=2, h=6, w=6, out.width='.8\\textwidth', cap='CA biplot of the suicide data using the contribution biplot scaling. Associations between the age-sex categories and the suicide methods can be read as the projections of the points on the vectors. The lengths of the vectors for the suicide categories reflect their contributions to this representation in a 2D plot. ', fig.pos='!htb'>>=
op <- par(cex=1.3, mar=c(4,4,1,1)+.1, lwd=2)
plot(suicide.ca, map="colgreen", arrows=c(FALSE, TRUE))
par(op)
@
The interpretation of the row points for the age--sex categories is similar to what we saw
earlier in \figref{fig:ca-suicide-plot}.  But now, the vectors for the suicide categories
reflect the contributions of those methods to the representation of association.
Thus, the methods \code{drown}, \code{gun} and \code{gas} have large contributions, while
\code{knife}, \code{hang}, and \code{poison} are relatively small.
Moreover, the projections of the points for the age--sex combinations on the method
vectors reflect the standardized residuals from independence.

The most comprehensive modern treatment of biplot methodology is the book
\emph{Understanding Biplots} \citep{Gower-etal:2011}.
Together with the book, they provide an \R package, \pkg{UBbipl},
that is capable of producing an astounding variety of high-quality plots.
Unfortunately, that package is only available on their publisher's web site%
\footnote{
\url{http://www.wiley.com/legacy/wileychi/gower/material.html}
}
and you need the book to be able to use it because all the documentation is in the book.
Nevertheless, we illustrate the use of the \func{cabipl} function to produce the
version of the CA biplot shown in \figref{fig:cabipl-suicide}.
<<cabipl-suicide, eval=FALSE>>=
library(UBbipl)
cabipl(as.matrix(suicide.tab),
    axis.col = gray(.4), ax.name.size=1,
    ca.variant = "PearsonResA",
    markers = FALSE,
    row.points.size = 1.5,
    row.points.col = rep(c("red", "blue"), 4),
    plot.col.points = FALSE,
    marker.col = "black", marker.size=0.8,
    offset = c(2, 2, 0.5, 0.5),
    offset.m = rep(-0.2, 14),
    output=NULL)
@
\begin{figure}[!htb]
\centering
\includegraphics[width=.8\textwidth]{ch06/fig/cabipl-suicide}
\caption{CA biplot of the suicide data, showing calibrated axes for the suicide methods.}
\label{fig:cabipl-suicide}
\end{figure}
This plot uses \code{ca.variant = "PearsonResA"} to specify that the biplot is to
approximate the standardized Pearson residuals by the inner product of each row
point on the vector for the column point for the suicide methods, as
also in \figref{fig:ca-suicide-biplot}.  However, \figref{fig:cabipl-suicide}
represents the methods calibrated axis lines, designed to be read as scales
for the projections of the row points (age--sex) on the methods.
The \Rpackage{UBbipl} has a huge number of options for controlling the details
of the biplot display.  See \citep[Ch. 2]{Gower-etal:2011} for all the details.


\end{Example}

\subsection{Biadditive biplots}
A different use of biplots for \ctabs stems from the close analogy between
additive relations for a quantitative response when there is no interaction
between factors, and the multiplicative relations for a \ctab when there is
no association.

For quantitative data \citet{BraduGabriel:78} show how the biplot can be used
to diagnose additive relations among rows and columns. For example, when
a two-way table is well-described by a two-factor ANOVA model with no
interaction,
\begin{equation*}%\label{eq:twoway}
y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}
\iff \mat{Y} \approx \vec{a} \vec{1}\trans + \vec{1} \vec{b}\trans
\end{equation*}
then, the row points, $\vec{a}_i$, and the column points, $\vec{b}_j$,
will fall on two straight lines at right angles to each other in the
biplot.
For a contingency table, the multiplicative relations among
frequencies under independence become additive relations
in terms of log frequency,
and \citet{Gabriel-etal:97} illustrate how biplots of log frequency
can be used to explore associations in two-way and three-way tables.

That is, For a two-way table, independence, $ A \perp B$,
implies that ratios of frequencies should be proportional for any
two rows, $i, i^{\prime}$ and any two columns, $j, j^{\prime}$.
%\begin{equation*}
%A \perp B \iff \frac{n_{ij}}{n_{i^{\prime} j}} = \frac{n_{ij^{\prime}}}{n_{i^{\prime} j^{\prime}}}
%\end{equation*}
Equivalently, this means that
the \IX{log odds ratio} for all such sets of four cells should
be zero:
\begin{equation*}
A \perp B \iff \log \theta_{i i^{\prime}, j j^{\prime}} = \log \left( \frac{n_{ij} n_{i^{\prime} j^{\prime}}} {n_{i^{\prime} j}  n_{ij^{\prime}}} \right) = 0
\end{equation*}
Now, if the log frequencies have been
centered by subtracting the grand mean,
\citet{Gabriel-etal:97} show that $\log \theta_{i i^{\prime}, j j^{\prime}}$
is approximated in the biplot (of $\log(n_{ij}) - \overline{ \log(n_{ij}) }$)
\begin{equation*}
\log \theta_{i i^{\prime}, j j^{\prime}} \approx
\vec{a}_i \trans \vec{b}_j - \vec{a}_{i^{\prime}} \trans \vec{b}_j -
\vec{a}_i \trans \vec{b}_{j^{\prime}} + \vec{a}_i \trans \vec{b}_{j^{\prime}}
= ( \vec{a}_i - \vec{a}_{i^{\prime}} )\trans ( \vec{b}_i - \vec{b}_{i^{\prime}} )
\end{equation*}

\begin{figure}[htb]
  \centering
  \includegraphics[width=.8\textwidth]{ch06/fig/bidemo}
  \caption[Independence implies orthogonal vector differences in a biplot of log frequency]{Independence implies orthogonal vector differences in a biplot of log frequency.  The line joining $\vec{a}_1$ to $\vec{a}_2$ represents
 $(\vec{a}_1 - \vec{a}_2)$.  This line is perpendicular to the line
 $(\vec{b}_1 - \vec{b}_2)$ under independence.}  \label{fig:bidemo}
\end{figure}

Therefore, this biplot criterion for independence in a two-way table
is whether
\(  ( \vec{a}_i - \vec{a}_{i^{\prime}} )\trans ( \vec{b}_i - \vec{b}_{i^{\prime}} ) \approx 0\) for all pairs of rows, $i, i^{\prime}$,
and all pairs of columns, $j, j^{\prime}$.
But \( ( \vec{a}_i - \vec{a}_{i^{\prime}} ) \) is the vector connecting
$\vec{a}_i$ to $\vec{a}_{i^{\prime}}$ and
 \( ( \vec{b}_j - \vec{b}_{j^{\prime}} ) \) is the vector connecting
$\vec{b}_j$ to $\vec{b}_{j^{\prime}}$, as shown in \figref{fig:bidemo},
and the inner product of any two vectors equals zero \emph{iff} they
are orthogonal.
Hence, this criterion implies that all
lines connecting pairs of row points are orthogonal to lines connecting
pairs of column points, as illustrated in \figref{fig:bidemo}.

\begin{Example}[soccer3]{UK Soccer scores}
We examined the data on UK Soccer scores in \exref{ex:soccer2}
and saw that the number of goals scored by the home and away teams
were largely independent (see \figref{fig:UKsoccer-mosaic}).
This \Dset provides a good test of the ability of the biplot
to diagnose independence.

<<biplot-soccer1>>=
data("UKSoccer", package="vcd")
dimnames(UKSoccer) <- list(Home=paste0("H", 0:4),
                           Away=paste0("A", 0:4))
@
Basic biplots in \R are provided by \func{biplot} that works mainly with
the result calculated by \func{prcomp} or \func{princomp}.
Here, we use \func{prcomp} on the log frequencies in the
\data{UKSoccer} table, adding 1, because there is one cell
with zero frequency.

<<biplot-soccer2>>=
soccer.pca <- prcomp(log(UKSoccer+1), center=TRUE, scale.=FALSE)
@
The result is plotted using a customized plot based on \func{biplot}
as shown in \figref{fig:biplot-soccer-plot}.
%%% Tricky here, showing the code for the figure in separate chunks, but compiling the figure
%%% in this chunk.

<<biplot-soccer-plot, echo=1:2, h=6, w=6, out.width='.7\\textwidth', cap='Biplot for the biadditive representation of independence for the UK Soccer scores. The row and column categories are independent in this plot when they appear as points on approximately orthogonal lines.', fig.pos='!htb' >>=
biplot(soccer.pca, scale=0, var.axes=FALSE,
  col=c("blue", "red"), cex=1.2, cex.lab=1.2,
  xlab="Dimension 1", ylab="Dimension 2")

# get the row and column scores
rscores <- soccer.pca$x[,1:2]
cscores <- soccer.pca$rotation[,1:2]
# means, excluding A2 and H2
rmean <- colMeans(rscores[-3,])[2]
cmean <- colMeans(cscores[-3,])[1]

abline(h=rmean, col="blue", lwd=2)
abline(v=cmean, col="red", lwd=2)
abline(h=0, lty=3, col="gray")
abline(v=0, lty=3, col="gray")
@
To supplement this plot and illustrate the orthogonality of row and column category points
under independence, we added horizontal and vertical lines as calculated below,
using the results returned by \func{prcomp}.  The initial version of this plot
showed that two points, A2 and H2 did not align with the others, so these were
excluded from the calculations.
<<biplot-soccer3, eval=FALSE >>=
# get the row and column scores
rscores <- soccer.pca$x[,1:2]
cscores <- soccer.pca$rotation[,1:2]
# means, excluding A2 and H2
rmean <- colMeans(rscores[-3,])[2]
cmean <- colMeans(cscores[-3,])[1]

abline(h=rmean, col="blue", lwd=2)
abline(v=cmean, col="red", lwd=2)
abline(h=0, lty=3, col="gray")
abline(v=0, lty=3, col="gray")
@

You can see that all the A points (except for A2) and all the H points (except for H2) lie along straight lines, and these lines are indeed at right angles,
signifying independence.
The fact that these straight lines are parallel to the coordinate axes is
incidental, and unrelated to the independence interpretation.

\end{Example}

\ixoff{biplot}

\section{Chapter summary}\label{sec:ca-summary}
\input{ch06/summary}

%\section{Further reading}\label{sec:ca-reading}

\section{Lab exercises}\label{sec:ca-lab}
<<exercises06, child="ch06/exercises.Rnw">>=
@

%\TODO{Cleanup local variables}
<<cleanup6, size='footnotesize', echo=FALSE>>=
#remove(list=objects(pattern="\\.tab|\\.df|\\.fit"))
.locals$ch06 <- setdiff(ls(), .globals)
#.locals$ch06
remove(list=.locals$ch06[sapply(.locals$ch06,function(n){!is.function(get(n))})])
@

