<<echo=FALSE>>=
source("Rprofile.R")
knitrSet("ch08")
#knitrSet("ch08", cache=TRUE)
require(vcdExtra, quietly = TRUE, warn.conflicts = FALSE)  # should go in Rprofile
.locals$ch08 <- NULL
@


\chapter{Loglinear and Logit Models for Contingency Tables}\label{ch:loglin}
%\input{ch08/vtoc}   %% visual contents images

\chapterprelude{
Loglinear models comprise another special case of generalized linear models
designed for \ctabs of frequencies.  They
are most easily interpreted through
visualizations, including mosaic displays and plots of associated
logit models.  
Special cases arise for ordered categorial variables and square tables
that allow more parsimonious models for associations.
% As with logistic regression, diagnostic plots
% and influence plots help to assure that the fitted model is
% an adequate summary of associations among variables.
}
% \minitoc
% \clearpage

\section{Introduction}\label{sec:loglin-intro}

\epigraph{Tables are like cobwebs, like the
sieve of Danaides; beautifully reticulated, orderly to look upon, but
which will hold no conclusion. Tables are abstractions, and the object a most
concrete one, so difficult to read the essence of.}{From \emph{Chartism} by Thomas Carlyle (1840), Chapter II, Statistics}

The chapter continues the modeling framework begun in \chref{ch:logistic},
and takes up the case of \loglin models for contingency tables of frequencies,
when all variables are discrete, another special case of
generalized linear models.
These models provide a comprehensive scheme to describe and
understand the associations among two or more categorical variables.
Whereas logistic regression models focus on the prediction of one response factor,
\loglin models treat all variables symmetrically, and attempt to
model all important associations among them.

In this sense,
\loglin models are analogous to a correlation analysis of continuous
variables, where the goal is to determine the patterns of dependence
and independence among a set of variables.  
When one variable is a response and the others are explanatory,
certain \loglin models are equivalent to logistic models for that
response.
Such models are also particularly useful
when there are two or more response variables, a case that would require
a multivariate version of the generalized linear model, for which
the current theory and implementations are thin at best.

\chref{ch:mosaic} and \chref{ch:corresp} introduced some basic aspects of
\loglin\ models in connection with mosaic displays and correspondence analysis.
\ix{correspondence analysis}
\ix{mosaic display}
In this chapter, the focus is on fitting and interpreting
\loglin\ models.
The usual analyses, with \func{loglm} and
\func{glm}
present the results in terms of tables of parameter
estimates.  Particularly for larger tables, it becomes difficult
to understand the nature of these associations from tables
of parameter estimates.  Instead, we emphasize plots of observed
and predicted frequencies,
probabilities or log odds (when there are one or more response
variables), as well as mosaic and other displays for interpreting a given
model.
% and residual and influence plots for model diagnostics.
We also illustrate how mosaic displays and \ca plots may be used
in a complementary way to the usual numerical summaries, to provide
additional insights into the data.

\secref{sec:loglin-counts} gives a brief overview of \loglin\
models in relation to the more familiar ANOVA and regression models
for quantitative data.
Methods and software for fitting these models are discussed in
\secref{sec:loglin-fitting}.
When one variable is a response, logit models for that response provide
a simpler, but equivalent means for interpreting and graphing
results of \loglin\ models, as we describe in \secref{sec:loglin-logit}.
Another class of simplified models (\secref{sec:loglin-ordinal})
occurs when one or more of the explanatory variables are ordinal,
and discrete levels might be replaced by numerical values.
Models for square tables (\secref{sec:loglin-square}), with the same row and column categories
comprise another special case giving simpler descriptions than the saturated model of
general association. These important special cases are extended to three-way and
higher-dimensional tables in \secref{sec:loglin-3wayord}.
Finally, \secref{sec:loglin-multiv} describes some methods for dealing with situations
where there are several response variables, and it is useful to understand both
the marginal relations of the responses with the predictors as well as how their
association varies with the predictors
%\TODO{Complete Chapter overview of sections}

\section{Loglinear models for frequencies}\label{sec:loglin-counts}

\Loglin\ models have been developed from two formally distinct,
but related perspectives.  The first is a discrete analog of familiar
ANOVA models
for quantitative data, where the multiplicative relations among joint and marginal probabilities are transformed into an additive one by transforming
the counts to logarithms.
The second is an analog of regression models, where the log of the
cell frequency is modeled as a linear function of discrete predictors,
with a random component often taken as the Poisson distribution
and called \term{Poisson regression}; this approach is treated in more detail as
generalized linear models for count data in \chref{ch:glm}.

\subsection{Loglinear models as ANOVA models for frequencies}
For two discrete variables, $A$ and $B$, suppose we have a multinomial sample of $n_{ij}$ observations in each cell $i,j$ of an $I \times J$
\ctab.   To ease notation, we replace a subscript by $+$ to represent
summation over that dimension, so that $n_{i+} = \Sigma_j n_{ij}$,
$n_{+j} = \Sigma_i n_{ij}$, and $n_{++} = \Sigma_{ij} n_{ij}$.

Let $\pi_{ij}$ be the joint probabilities in the table, and let
$m_{ij} = n_{++} \pi_{ij}$ be the expected cell frequencies under
any model.
Conditional on the observed total count, $n_{++}$,
each count has a Poisson distribution, with mean $m_{ij}$.
Any \loglin\ model may be expressed as a linear model for the $\log m_{ij}$.
For
example, the hypothesis of independence means that the expected
frequencies, \(m_{ij}\), obey
\begin{equation*}%\label{eq:indep}
  m_{ij} = \frac{ m_{i+} \:  m_{+j} } {m_{++}}
  \period
\end{equation*}

This multiplicative model can be transformed to an additive (linear)
model by taking logarithms of both sides:
\begin{equation*}
  \log ( m_{ij} ) = \log ( m_{i+} )  +  \log ( m_{+j} )
 - \log ( m_{++} )
 \comma
\end{equation*}
which is usually expressed in an equivalent form in terms of model
parameters,
\begin{equation} \label{eq:lmain}
\log ( m_{ij} ) = \mu  +  \lambda_i^A +  \lambda_j^B
\end{equation}
where \(\mu\) is a function of the total sample size, \(\lambda_i^A\)
is the ``main effect'' for variable A,
\(\lambda_i^A = \log \pi_{i+} - \sum_k(\log \pi_{k+}) / I \),
 and \(\lambda_j^B\) is the
``main effect'' for variable B,
\(\lambda_j^B = \log \pi_{+j} - \sum_k(\log \pi_{+k}) / J \).
Model \eqref{eq:lmain} is called the \term{loglinear independence model}
for a two-way table.

In this model, there
are $1 + I + J$ parameters,  but only $(I-1)+(J-1)$ are separately
estimable. Hence,
the typical ANOVA sum-to-zero
restrictions
are usually applied to the parameters:
\begin{equation*}
\sum_i^I \:
\lambda_i^A  = \sum_j^J \:  \lambda_j^B = 0 \period
\end{equation*}
These ``main effects'' in
\loglin models pertain to differences among the marginal
probabilities of a variable (which are usually not of direct interest).

Other restrictions to make the parameters identifiable are also used.
Setting the first values,
$\lambda_1^A$ and $\lambda_1^B$
to zero (the default in \func{glm}), defines
$\lambda_i^A = \log \pi_{i+} - \log \pi_{1+}$, and
$\lambda_j^B = \log \pi_{+j} - \log \pi_{+1}$,
as deviations from the first, reference category,
but these parameterizations are otherwise identical.
For modeling functions in \R (\func{lm}, \func{glm}, etc.)
the reference category parameterization is obtained using
\func{contr.treatment}, while the sum-to-zero constraints
are obtained with \func{contr.sum}.

Model \eqref{eq:lmain} asserts that the row and column variables
are independent.
For a two-way table, a model that allows an arbitrary
association between the variables is the \term{saturated model},
including an additional term, $\lambda_{ij}^{AB}$:
\begin{equation}\label{eq:lsat}
\log ( m_{ij} ) = \mu  +  \lambda_i^A
+  \lambda_j^B  +  \lambda_{ij}^{AB} \comma
\end{equation}
where again, restrictions must be imposed for estimation:
\begin{equation}\label{eq:lrestrict}
\sum_i^I \,  \lambda_i^A  = 0, \quad
\sum_j^J \,  \lambda_j^B = 0, \quad
\sum_i^I \,  \lambda_{ij}^{AB} =
\sum_j^J \,  \lambda_{ij}^{AB} = 0  \period
\end{equation}
There are thus $I-1$ linearly independent $\lambda_i^A$ row parameters,
$J-1$ linearly independent $\lambda_j^B$ column parameters,
and $(I-1)(J-1)$ linearly independent $\lambda_{ij}^{AB}$ association  parameters.
This model is called the \emph{saturated model} because the number of
parameters in $\mu$, $\lambda_i^A$, $\lambda_j^B$, and $\lambda_{ij}^{AB}$
is equal to the number of frequencies in the two-way table,
\begin{equation*}
  \cwe{1}{\mu} + \cwe{I-1}{\lambda_i^A} + \cwe{J-1}{\lambda_j^B} + \cwe{(I-1)(J-1)}{\lambda_{ij}^{AB}}
  = \cwe{I J}{n_{ij}}
\end{equation*}
The association parameters $\lambda_{ij}^{AB}$ express the departures from independence,
so large absolute values pertain to cells that differ from the independence model.

Except for the difference in notation, model \eqref{eq:lsat} is formally the same
as a two-factor ANOVA model with an interaction, typically expressed as
$  E ( y_{ij} ) = \mu  +  \alpha_i  +  \beta_j +(\alpha  \beta)_{ij}
$.
Hence, associations between variables in \loglin models are
analogous to interactions in ANOVA models.
The use of superscripted symbols,
$\lambda_i^A, \lambda_j^B , \lambda_{ij}^{AB}$ rather than separate
Greek letters is a convention in \loglin models, and useful mainly
for \mway\ tables.

Models such as \eqref{eq:lmain} and \eqref{eq:lsat} are
examples of \term{hierarchical models}.
This means that the model must contain all lower-order terms contained
within any high-order term in the model.
Thus, the saturated model, \eqref{eq:lsat} contains $\lambda_{ij}^{AB}$,
and therefore \emph{must} contain $\lambda_i^A $ and $\lambda_j^B$.
As a result, hierarchical models may be identified by the shorthand
notation which lists only the high-order terms: model \eqref{eq:lsat}
is denoted $[A B]$, while model \eqref{eq:lmain} is $[A] [B]$.

\subsection{\Loglin\ models for three-way tables}\label{sec:loglin-3way}
\Loglin models for three-way \ctabs
were described briefly in \secref{sec:mosaic-fitting}.
Each type of model allows associations among different sets of variables
and each has a different independence interpretation, as illustrated in
\tabref{tab:hyp3way}.

For a three-way table, the saturated model, denoted $[ABC]$ is
\begin{equation} \label{eq:lsat3}
  \log \,  m_{ijk}  =
  \mu  +  \lambda_i^A
  +  \lambda_j^B
  +  \lambda_k^C
  +  \lambda_{ij}^{AB}
  +  \lambda_{ik}^{AC}
  +  \lambda_{jk}^{BC}
  +  \lambda_{ijk}^{ABC}
  \period
\end{equation}
This model allows all variables to be associated; \eqref{eq:lsat3} fits the data perfectly because
the number of independent parameters equals the number of table cells.
Two-way terms, such as $\lambda_{ij}^{AB}$ pertain to the
\emph{conditional association} between pairs of factors,
controlling for the remaining variable.
The presence of the three-way term, $\lambda_{ijk}^{ABC}$,
means that the partial association (conditional odds ratio) between any pair
varies over the levels of the third variable.

Omitting the three-way term in Model \eqref{eq:lsat3} gives the model
$[AB] [AC] [BC]$,
\begin{equation} \label{eq:lno3way}
  \log \,  m_{ijk}  =
  \mu  +  \lambda_i^A
  +  \lambda_j^B
  +  \lambda_k^C
  +  \lambda_{ij}^{AB}
  +  \lambda_{ik}^{AC}
  +  \lambda_{jk}^{BC}
  \comma
\end{equation}
in which all pairs are conditionally dependent given the remaining one.
For any pair,
the conditional odds ratios are the \emph{same} at all levels of the remaining
variable, so this model is often called the \term{homogeneous association model}.

The interpretation of terms in this model may be illustrated
using the Berkeley admissions data (\exref{ex:berkeley2} and \exref{ex:berkeley3}), for which the factors are Admit,
 Gender, and Department, in a $2 \times 2 \times 6$ table.
In the homogeneous association model,
\begin{equation}\label{eq:berk1}
  \log \,  m_{ijk}  =
  \mu  +  \lambda_i^A
  +  \lambda_j^D
  +  \lambda_k^G
  +  \lambda_{ij}^{AD}
  +  \lambda_{ik}^{AG}
  +  \lambda_{jk}^{DG}
  \comma
\end{equation}
the $\lambda$-parameters have the following interpretations:
\begin{itemize}
\item The main effects, $\lambda_i^A , \lambda_j^D$ and $\lambda_k^G$
   pertain to differences in the one-way marginal probabilities.
    Thus $\lambda_j^D$ relates to differences in the total number of applicants
    to these departments, while $\lambda_k^G$ relates to the differences
    in the overall numbers of men and women applicants.
\item $\lambda_{ij}^{AD}$ describes the conditional association between
   admission and department, that is  different admission rates across
   departments (controlling for gender).
\item $\lambda_{ik}^{AG}$ relates to the conditional association between
   admission and gender, controlling for department.
    This term, if significant, might be interpreted as indicating
   gender-bias in admissions.
\item $\lambda_{jk}^{DG}$, the association between
 department and gender, indicates whether males and females apply
 differentially across departments.
\end{itemize}

As we discussed earlier (\secref{sec:mosaic-threeway}), \loglin models
for three-way (and larger) tables often have an interpretation in terms
of various types of independence relations illustrated in \tabref{tab:hyp3way}.
The model \eqref{eq:lno3way} has no such interpretation, however the smaller
model $\LLM{AC, BC}$ can be interpreted as asserting that $A$ and $B$ are
(conditionally) independent controlling for $C$;  this independence interpretation
is symbolized as $A \perp B \given C$.
Similarly, the model $\LLM{AB, C}$ asserts that $A$ and $B$ are jointly
independent of $C$: $(A, B) \perp C$, while the model $\LLM{A, B, C}$
is the model of mutual (complete) independence, $A \perp B \perp C$.


\subsection{Loglinear models as GLMs for frequencies}\label{sec:loglin-glms}
In the GLM approach, a \loglin model may be cast in the form of a regression
model for $\log \vec{m}$, where the table cells are reshaped to a
column vector.
One advantage is that models for tables of any size and structure
may be expressed in a compact form.

For a contingency table of variables $A, B, C,\cdots $, with $N=I\times
J\times K\times \cdots $ cells, let $\vec{n}$ denote a column vector of
the observed counts arranged in standard order, and let $\vec{m}$ denote
a similar vector of the expected frequencies under some model. Then \emph{any}
\loglin model may be expressed
in the form
\begin{equation*}
 \log \vec{m} = \mat{X}\vec{\beta}
 \comma
\end{equation*}
where $\mat{X}$ is a known design or \term{model matrix} and $\vec{\beta}$ is a
column vector containing the unknown $\lambda $ parameters.

For example, for
a $2\times 2$ table, the saturated model \eqref{eq:lsat} with the usual zero-sum constraints \eqref{eq:lrestrict}
can be represented as
\begin{equation*}
\log \left(
\begin{array}{c}
 m_{11} \\
 m_{12} \\
 m_{21} \\
 m_{22}
\end{array}
\right) =\left[
\begin{array}{rrrr}
1 & 1 & 1 & 1 \\
1 & 1 & -1 & -1 \\
1 & -1 & 1 & -1 \\
1 & -1 & -1 & 1
\end{array}
\right] \left(
\begin{array}{c}
\mu  \\
\lambda _1^A \\
\lambda _1^B \\
\lambda _{11}^{AB}
\end{array}
\right)
\end{equation*}
Note that only the linearly independent parameters are represented here.
$\lambda_2^A = - \lambda_1^A$, because $\lambda_1^A + \lambda_2^A =0$, and
$\lambda_2^B = - \lambda_1^B$, because $\lambda_1^B + \lambda_2^B =0$,
and so forth.

An additional substantial advantage of the GLM formulation is that it makes it easier
to express models with ordinal or quantitative variables.
\func{glm}, with a model formula of the form
\verb|Freq ~ .| involving factors $A, B, \dots$ and quantitative variables
$x_1, x_2, \dots$,
constructs the model matrix $\mat{X}$ from the terms given in the formula.
A factor with $K$ levels gives rise to $K-1$ columns
for its main effect and sets of $K-1$ columns in each interaction effect.
A quantitative predictor, say $x_1$ (with a linear effect)
creates a single column with its values
and interactions with other terms are calculated at the products
of the columns for the main effects.

The parameterization for factors is controlled by
the contrasts assigned to a given factor (if any), or by the general
\code{contrasts} option, that gives the contrast functions used
for unordered and ordered factors:
<<contrasts>>=
options("contrasts")
@
\noindent This says that, by default, unordered factors use
the baseline (first) reference-level parameterization, while
ordered factors are given a parameterization based on orthogonal
polynomials, allowing linear, quadratic, ... effects, assuming
integer-spacing of the factor levels.



\section{Fitting and testing \loglin models} \label{sec:loglin-fitting}

For a given table, possible \loglin models range from the
baseline model of mutual independence, $\LLM{A, B, C, \dots}$
to the saturated model, $\LLM{A B C \dots}$ that fits the
observed frequencies perfectly, but offers no simpler description or
interpretation than the data itself.

Fitting a \loglin model
is usually a process of deciding which association terms are large enough
(``significantly different from zero'') to warrant inclusion
in a model to explain the observed frequencies.
Terms which are excluded from the model go
into the residual or error term, which reflects the overall
badness-of-fit of the model.  The usual goal of \loglin modeling
is to find a small model (few association terms) which nonetheless achieves a
reasonable fit (small residuals).

\subsection{Model fitting functions} \label{sec:loglin-functions}
In \R, the most basic function for fitting \loglin models is
\func{loglin} in the \Rpackage{stats}.
This uses the classical iterative proportional fitting (IPF) algorithm
described in \citet{Haberman:1972} and \citet[\S 3.4]{Fienberg:80}.
It is designed to work with the frequency data in table form,
and a model specified in terms of the (high-order) table margins
to be fitted. For example, the model \eqref{eq:lno3way}
of homogenous association for a three-way table is specified
as
<<loglm1, eval=FALSE>>=
loglin(mytable, margin=list(c(1, 2), c(1, 3), c(2, 3)))
@

The function \func{loglm} in \pkg{MASS} provides a more convenient
front-end to \func{loglin}
to allow \loglin models to be specified using a model formula.
With table variables \code{A}, \code{B} and \code{C}, the same model can be
fit using \func{loglm} as
<<loglm2, eval=FALSE>>=
loglm(~ (A + B + C)^2, data=mytable)
@
When the data is a frequency data frame with frequencies in \code{Freq},
for example, the result of \code{mydf <- as.data.frame(mytable)},
you can also use a two-sided formula:
<<loglm3, eval=FALSE>>=
loglm(Freq ~ (A + B + C)^2, data=mydf)
@

As implied in \secref{sec:loglin-glms}, \loglin models can also be fit
using \func{glm}, using \code{family=poisson} which constructs the
model for \code{log(Freq)}.  The same model is fit with \func{glm}
as:
<<loglm4, eval=FALSE>>=
glm(Freq ~ (A + B + C)^2, data=mydf, family=poisson)
@
While all of these fit equivalent models, the details of the printed output,
model objects, and available methods differ, as indicated in some of the examples
that follow.

It should be noted that both the \func{loglin}/\func{loglm} methods based
on iterative proportional fitting, and the \func{glm} approach using
the Poisson model for log frequency give maximum likelihood estimates,
$\widehat{\vec{m}}$, of the expected frequencies, as long as all
observed frequencies $\vec{n}$ are \emph{all} positive.
Some special considerations when there cells with
zero frequencies are described in \secref{sec:loglin-zeros}.


\subsection{Goodness-of-fit tests} \label{sec:loglin-goodfit}

For an \nway table, global goodness-of-fit tests for a \loglin model
attempt to answer the question ``How well does the model reproduce
the observed frequencies?'' That is, how close are the fitted frequencies
estimated under the model to those of the saturated model or the data?

To avoid multiple subscripts for an \nway table,
let $\vec{n} = n_1, n_2, \ldots , n_N$ denote
the observed frequencies in a table with $N$ cells, and
corresponding fitted frequencies
$\widehat{\vec{m}} = \widehat{m}_1, \widehat{m}_2, \ldots , \widehat{m}_N$
according to a particular \loglin model.
The standard goodness-of-fit statistics are sums over the cells
of measures of the difference between the $\vec{n}$ and $\widehat{\vec{m}}$.

The most commonly used are the familiar Pearson chi-square,
\begin{equation}\label{eq:pchi}
X^2 = \sum_i^N \frac{( n_i - \widehat{m}_i )^2}{\widehat{m}_i}
\comma
\end{equation}
and the \LR\ \GSQ\ or \term{deviance} statistic,
\begin{equation}\label{eq:pgsq}
\GSQ =  2 \sum_i^N n_i \, \log \left( \frac{n_i} {\widehat{m}_i} \right)
\period
\end{equation}
Both of these statistics have asymptotic \chisq
distributions (as $\Sigma \vec{n} \to \infty$), reasonably well-approximated
when all expected frequencies are large.%
\footnote{
% A wider class of test statistics including \chisq\
% and \GSQ\ as special cases is
% described by \citet{CressieRead:84} and \citet{ReadCressie:88}.
Except in bizarre or borderline
cases, these tests provide the same conclusions when
expected frequencies are at least moderate (all $\widehat{\vec{m}} > 5$).
However, \GSQ approaches the theoretical chi-squared distribution
more slowly than does \chisq, and the approximation may be poor
when the average cell frequency is less than 5.
}
The (residual) degrees of freedom are the number of cells ($N$) minus the
number of estimated parameters.
The \LR test can also be expressed as twice the difference
in log-likelihoods under saturated and fitted models,
\begin{equation*}%\label{eq:lrt}
% \GSQ =  -2 \log \left[ \frac{\mathcal{L}(\widehat{\vec{m}}; \vec{n})}
%                             {\mathcal{L}(\vec{n}; \vec{n})}  \right]
%      =  -2 [ \log \mathcal{L}(\widehat{\vec{m}}; \vec{n})
%            - \log \mathcal{L}(\vec{n}; \vec{n}) ]
\GSQ =  2 \log \left[ \frac{\mathcal{L}(\vec{n}; \vec{n})}  {\mathcal{L}(\widehat{\vec{m}}; \vec{n})}
               \right]
     =  2 [ 
           \log \mathcal{L}(\vec{n}; \vec{n}) 
         - \log \mathcal{L}(\widehat{\vec{m}}; \vec{n})
          ]
\comma
\end{equation*}
where 
% $\mathcal{L}(\widehat{\vec{m}}; \vec{n})$ is the maximized likelihood
% for the fitted model and 
% $\mathcal{L}(\vec{n}; \vec{n})$ is the corresponding
% likelihood for the saturated model.
% 
$\mathcal{L}(\vec{n}; \vec{n})$ is the
likelihood for the saturated model and
$\mathcal{L}(\widehat{\vec{m}}; \vec{n})$ is the corresponding maximized likelihood
for the fitted model.


In practice such global tests are less useful for comparing competing models.
You may  find that several different models have an acceptable fit
or, sadly, that none do (usually because you are ``blessed'' with a
large sample size).
It is then helpful to compare competing models \emph{directly},
and two strategies are particularly useful in these cases.

First, the \LR\ \GSQ\ statistic has the property
in that one can compare two
\term{nested models} by their difference in \GSQ statistics,
which has a \chisq distribution on the difference in degrees of
freedom.
Two models, $M_1$ and $M_2$, are nested when one, say, $M_2$, is
a special case of the other.  That is, model $M_2$ (with $\nu_2$ residual df)
contains a subset of
the parameters of $M_1$ (with $\nu_1$ residual df),
the remaining ones being effectively set to zero.
Model $M_2$ is therefore more restrictive and cannot fit the data better
than the more general model $M_1$, i.e., $\GSQ (M_2) \ge \GSQ (M_2)$.
The least restrictive of all models, with $\GSQ = 0$ and $\nu=0$ df is
the saturated model for which $\widehat{\vec{m}} = \vec{n}$.

Assuming that the less restrictive model $M_1$ fits, the difference in
\GSQ,
\begin{eqnarray}
\Delta \GSQ \equiv \GSQ ( M_2 \given M_1 )
& = & \GSQ ( M_2 ) - \GSQ ( M_1 ) \label{eq:gsqnest1} \\
& = & 2 \sum_i n_i \, \log ( \widehat{m}_{i1} / \widehat{m}_{i2} ) \label{eq:gsqnest2}
\end{eqnarray}
has a chi-squared distribution with df = $\nu_2 - \nu_1$.
The last equality \eqref{eq:gsqnest2} follows from substituting in \eqref{eq:pgsq}.

Rearranging terms in \eqref{eq:gsqnest1}, we see that we can partition
the $\GSQ ( M_2 )$ into two terms,
\begin{equation*}
\GSQ ( M_2 ) = \GSQ ( M_1 ) + \GSQ ( M_2 \given M_1 )
\period
\end{equation*}
The first term measures the difference between the data and the more
general model $M_1$.  If this model fits, the second term measures the
additional lack of fit imposed by the more restrictive model.
In addition to providing a more focused test, $\GSQ ( M_2 \given M_1 )$
also follows the chi-squared distribution more closely when some
$\{ m_i \}$ are small
\citep[\S 10.6.3]{Agresti:2013}.

Alternatively, a second strategy uses other measures that combine
goodness-of-fit with model parsimony and may also be used to compare
non-nested models.  The statistics described below are all cast in
the form of badness-of-fit relative to degrees of freedom, so that
smaller values reflect ``better'' models.

The simplest idea \citep{Goodman:71}
is to use $\GSQ / df$
(or $\chisq / df$), which has an asymptotic expected value of 1 for a good-fitting
model.  This type of measure is not routinely reported by \R software, but is
easy to calculate from output.

The \term{Akaike Information Criterion} (AIC) statistic
\ix{AIC}
\citep{Akaike:73} is a very general criterion for model selection
with maximum likelihood estimation, based on the idea of maximizing
the information provided by a fitted model.  AIC is defined generally
as
\begin{equation*}
\mbox{AIC} = -2 \log \mathcal{L} + 2 k
\end{equation*}
where $\log \mathcal{L}$ is the maximized log likelihood and $k$ is
the number of parameters estimated in the model.
Better models correspond to \emph{smaller} AIC.  For \loglin models,
minimizing AIC is equivalent to minimizing
\begin{equation*}
\mbox{AIC}^{\star} = \GSQ - 2 \, \nu \comma
\end{equation*}
where $\nu$ is the residual df,
but the values of AIC and AIC$^\star$ differ by an arbitrary constant.
This form is easier to calculate by hand
from the output of any modeling function if AIC is not reported,
or an \func{AIC} method is not available.

A third statistic of this type is the \term{Bayesian Information Criterion}
(BIC) due to \citet{Schwartz:78} and \citet{Raftery:1986},
\ix{BIC}
\begin{equation*}
\mbox{BIC} = \GSQ - \log (n) \, \nu \comma
\end{equation*}
where $n$ is the total sample size.  Both AIC and BIC penalize the fit
statistic for increasing number of parameters.
BIC also penalizes the fit directly with (log) sample size, and so expresses
a preference for less complex models than AIC as the sample size increases.

\subsection{Residuals for \loglin models}\label{sec:loglin-residuals}

Test statistics such as \GSQ can determine whether a model has significant
lack of fit, and model comparison tests using $\Delta \GSQ = \GSQ (M_2 \given M_1)$
can assess whether the extra term(s) in model $M_1$ significantly improves the model
fit.  Beyond these tests, the pattern of residuals for individual cells
offers important clues regarding the nature of lack of fit and can help suggests
associations that could be accounted for better.

As with logistic regression models (\secref{sec:logist-resids}),
several types of residuals are available for
\loglin models. For cell $i$ in the vector form of the \ctab, the
\term{raw residual} is simply the difference between the observed and fitted
frequencies, $e_i = n_i - \widehat{m}_i$.

The \term{Pearson residual} is the square root of the contribution of the cell
to the Pearson \chisq,
\begin{equation}\label{eq:reschi2}
r_i = \frac{n_i - \widehat{m}_i}{\sqrt{\widehat{m}_i}}
\end{equation}
Similarly, the \term{deviance residual} can be defined as
\begin{equation}\label{eq:resdev2}
g_i = \sign({n_i - \widehat{m}_i}) \sqrt{2 n_i \log (n_i / \widehat{m}_i) - 2 (n_i - \widehat{m}_i) }
\end{equation}

Both of these attempt to standardize the distribution of the residuals to a standard normal,
$N (0,1)$ form.  However, as pointed out by \citet{Haberman:73}, the asymptotic variance
of these is less than one (with average value $df/N$)
but, worse--- the variance decreases with $\widehat{m}_i$.
That is, residuals for cells with small expected frequencies have larger sampling variance,
as might be expected.

Consequently, Haberman suggested dividing the Pearson residual by its estimated standard
error, giving what are often called \term{adjusted residual}s.
When \loglin models are fit using the GLM approach, the adjustment
may be calculated using the leverage (``hat value''), $h_i$ to give
appropriately standardized residuals,
\begin{eqnarray*}
r_i^\star & = & r_i / \sqrt{1 - h_i} \\
g_i^\star & = & g_i / \sqrt{1 - h_i}
\end{eqnarray*}
These standardized versions are generally preferable, particularly for visualizing
model lack of fit using mosaic displays.
The reason for preferring adjusted residuals is illustrated in
\figref{fig:stres-plot}, a plot of the factors, $\sqrt{1 - h_i}$,
determining the standard errors of the residuals against the
fitted values, $\widehat{m_i}$, in the model for the
\data{UCBAdmissions} data described in \exref{ex:berkeley6} below.
The values shown in this plot are calculated as:

<<stres-plot0>>=
berkeley <- as.data.frame(UCBAdmissions)
berk.glm1 <- glm(Freq ~ Dept * (Gender+Admit), data=berkeley, family="poisson")
fit <- fitted(berk.glm1)
hat <- hatvalues(berk.glm1)
stderr <- sqrt(1-hat)
@
<<stres-plot, echo=FALSE, h=6, w=7, out.width='.7\\textwidth', cap='Standard errors of residuals, $\\sqrt{1-h_i}$ decrease with expected frequencies. This plot shows why ordinary Pearson and deviance residuals may be misleading.  The symbol size in the plot is proportional to leverage, $h_i$. Labels abbreviate Department, Gender and Admit, colored by Admit.'>>=
op <- par(mar=c(5,4,1,1)+.1)
plot(fit, stderr, cex = 5*hat,
  ylab="Std. Error of Residual", xlab="Fitted Frequency",
  cex.lab=1.2)
labs <- with(berkeley,
  paste(Dept, substr(Gender,1,1), ifelse(Admit=="Admitted", "+", "-"), sep=""))
col <- ifelse(berkeley$Admit=="Admitted", "blue", "red")
text(fit, stderr, labs, col=col, cex=1.2)
par(op)
@


In \R, raw, Pearson and deviance residuals may be obtained using
\code{residuals(model, type=)}, where \code{type} is one of
\code{"raw"}, \code{"pearson"} and \code{"deviance"}.
Standardized (adjusted) residuals can be calculated using
\code{rstandard(model, type=)}, for \code{type="pearson"} and \code{type="deviance"}
versions.



\subsection[Using loglm()]{Using \func{loglm}}\label{loglin-loglin}
Here we illustrate the basics of fitting \loglin models using
\func{loglm}.  As indicated in \secref{sec:loglin-functions},
the model to be fitted is specified by a model formula
involving the table variables.
The \Rpackage{MASS} provides a \func{coef} method for \class{loglm} objects
that extracts the estimated parameters and a \func{residuals} method that
calculates various types of residuals according to a \code{type}
argument, one of \code{"deviance", "pearson", "response"}.
\pkg{vcd} and \pkg{vcdExtra} provide a variety of plotting methods,
including \func{assoc}, \func{sieve}, \func{mosaic} and \func{mosaic3d}
for \class{loglm} objects.


\begin{Example}[berkeley5]{Berkeley admissions}
The \data{UCBAdmissions}
on admissions to the six largest graduate departments
at U.C. Berkeley
was examined using graphical methods
in \chref{ch:twoway} (\exref{ex:berkeley3})
and in \chref{ch:mosaic} (\exref{ex:berkeley4}).
We can fit and compare several \loglin models
as shown below.

The model of mutual independence, $\LLM{A, D, G}$,
is not substantively reasonable here,
because the association of \var{Dept} and \var{Gender}
should be taken into account to control for these
variables,
but we show it here to illustrate the form of the printed output, giving
the Pearson \chisq and \LR \GSQ tests of goodness of fit,
as well as some optional arguments for saving additional
components in the result.
<<berk-loglm0>>=
data("UCBAdmissions")
library(MASS)
berk.loglm0 <- loglm(~ Dept + Gender + Admit, data=UCBAdmissions,
                     param=TRUE, fitted=TRUE)
berk.loglm0
@
The argument \code{param=TRUE} stores the estimated parameters
in the \loglin model and \code{fitted=TRUE} stores the
fitted frequencies $\widehat{m}_{ijk}$.
The fitted frequencies
can be extracted from the model object using \func{fitted}.
<<berk-loglm0-1, R.options=list(digits=4)>>=
structable(Dept ~ Admit+Gender, fitted(berk.loglm0))
@

Similarly, you can extract the estimated parameters with \code{coef(berk.loglm0)},
and the Pearson residuals with \code{residuals(berk.loglm0, type="pearson")}.

Next, consider the
model of conditional independence of gender and admission given department,
$\LLM{AD, GD}$ that allows associations of admission with department and
gender with department.
<<berk-loglm1>>=
# conditional independence in UCB admissions data
berk.loglm1 <- loglm(~ Dept * (Gender + Admit), data=UCBAdmissions)
berk.loglm1
@
Finally for this example, the model of homogeneous association,
$\LLM{AD, AG, GD}$ can be fit as follows.%
\footnote{It is useful to note here
that the added term $[AG]$ allows a general association of admission
with gender (controlling for department). A significance test for this term,
or for model \code{berk.loglm2} against \code{berk.loglm1}
is a proper test for the assertion of gender bias in admissions.
}
<<berk-loglm2>>=
berk.loglm2 <-loglm(~(Admit + Dept + Gender)^2, data=UCBAdmissions)
berk.loglm2
@

Neither of these models fits particularly well, as judged by the goodness-of-fit
Pearson \chisq and \LR \GSQ test against the saturated model.
The \func{anova} method for a nested collection of \class{loglm} models
gives a series of \LR tests of the difference, $\Delta \GSQ$
between each sequential pair of models according to \eqref{eq:gsqnest1}.

<<berk-loglm-anova>>=
anova(berk.loglm0, berk.loglm1, berk.loglm2, test="Chisq")
@
The conclusion from these results is that
the model \code{berk.loglm1}
is not much worse than model \code{berk.loglm2}, but there is still significant
lack-of-fit.  The next example, using \func{glm}, shows how to visualize the
lack of fit and account for it.

\end{Example}

\subsection[Using glm()]{Using \func{glm}}\label{sec:loglin-glm}
\Loglin models fit with \func{glm} require the data in a data frame
in frequency form, for example as produced by \func{as.data.frame} from
a table.
The model formula expresses the model for the frequency variable, and
uses \code{family=poisson} to specify the error distribution.
More general distributions for frequency data are discussed in
\chref{ch:glm}.

\begin{Example}[berkeley6]{Berkeley admissions}

For the $2 \times 2 \times 6$ \data{UCBAdmissions} table,
first transform this to a frequency data frame:
<<>>=
berkeley <- as.data.frame(UCBAdmissions)
head(berkeley)
@

Then, the model of conditional independence
corresponding to \code{berk.loglm1} can be fit using \func{glm}
as shown below.
<<berk-glm1>>=
berk.glm1 <- glm(Freq ~ Dept * (Gender+Admit),
                 data=berkeley, family="poisson")
@
Similarly, the all two-way model of homogeneous association is fit using
<<berk-glm2>>=
berk.glm2 <- glm(Freq ~ (Dept + Gender + Admit)^2,
                 data=berkeley, family="poisson")
@
These models are equivalent to those fit using \func{loglm} in \exref{ex:berkeley5}.
We get the same residual \GSQ as before, and the \LR test of $\Delta \GSQ$ given
by \func{anova} gives the same result, that the model \code{berk.glm2}
offers no significant improvement over model \code{berk.glm1}.

<<berk-glm-anova>>=
anova(berk.glm1, berk.glm2, test="Chisq")
@

Among other advantages of using \func{glm} as opposed to \func{loglm} is that
an \func{anova} method is available for \emph{individual} \class{glm} models, giving
significance tests of the contributions of each \emph{term} in the model,
as opposed to the tests for individual coefficients provided by \func{summary}.%
\footnote{
Unfortunately, in the historical development of \R, the \func{anova} methods for
linear and generalized linear
models provide only \emph{sequential} (``Type I'') tests that are computationally easy,
but useful only under special circumstances.  The \Rpackage{car} provides
an analogous \func{Anova} method that gives more generally useful
\emph{partial} (``Type II'') tests
for the additional contribution of each term beyond the others,
taking marginal relations into account.
}
<<berk-glm1-anova>>=
anova(berk.glm1, test="Chisq")
@

We proceed to consider what is wrong with these models and how they can be improved.
A mosaic display can help diagnose the reason(s) for lack of fit of these models.
We focus here on the model $\LLM{AD, GD}$ that allows an association between
gender and department (i.e., men and women apply at different rates to departments).

The \func{mosaic} method for \class{glm} objects in \pkg{vcdExtra}
provides a \code{residuals\_type} argument, allowing \code{residuals\_type="rstandard"}
for standardized residuals.  The \code{formula} argument here
pertains to the order of the variables in the mosaic, not a model formula.
<<berk-glm1-mosaic, fig.pos='!htb', h=6, w=6, out.width='.7\\textwidth', cap='Mosaic display for the model [AD][GD], showing standardized residuals for the cell contributions to \\GSQ', scap='Mosaic display for the model AD GD, showing standardized residuals'>>=
library(vcdExtra)
mosaic(berk.glm1, shade=TRUE, formula=~Admit+Dept+Gender,
       residuals_type="rstandard", labeling=labeling_residuals,
       main="Model: [AdmitDept][GenderDept]")
@
The mosaic display, shown in \figref{fig:berk-glm1-mosaic}, indicates that this model fits well
(residuals are small) except in Department A.
This suggests a model which allows an association between Admission and Gender in Department
A only,
\begin{equation}\label{eq:berk2}
  \log \,  m_{ijk}  =
  \mu
  +  \lambda_i^A
  +  \lambda_j^D
  +  \lambda_k^G
  +  \lambda_{ij}^{AD}
  +  \lambda_{jk}^{DG}
  +  I(j=1) \lambda_{ik}^{AG} \comma
\end{equation}
where the indicator function
$I(j=1) $ equals 1 for Department A ($j=1$) and is zero otherwise.
This model asserts that Admission and Gender are conditionally independent,
given Department, except in Department A.  It has one more parameter than
the conditional independence model, $[AD] [GD]$, and forces perfect fit
in the four cells for Department A.

Model \eqref{eq:berk2} may be fit with \func{glm} by constructing a variable
equal to the interaction of \var{gender} and \var{admit}
with a dummy variable having the value 1 for Department A and 0 for other departments.
<<berk-glm3>>=
berkeley <- within(berkeley,
                   dept1AG <- (Dept=='A')*(Gender=='Female')*(Admit=='Admitted'))
head(berkeley)
@
Fitting this model with the extra term \code{dept1AG} gives \code{berk.glm3}
<<berk-glm4, size='footnotesize'>>=
berk.glm3 <- glm(Freq ~ Dept * (Gender+Admit) + dept1AG,
                 data=berkeley, family="poisson")
@
This model does indeed fit well, and represents a substantial improvement
over model \code{berk.glm1}:
<<berk-glm5>>=
vcdExtra::Summarise(berk.glm3)
anova(berk.glm1, berk.glm3, test="Chisq")
@
The parameter estimate for the \code{dept1AG} term,
$\widehat{\lambda}_{ik}^{AG} = 1.052$ may be interpreted as
the log odds ratio of admission for females as compared to males in Dept. A.
The odds ratio is $\exp(1.052) = 2.86$, the same as the value calculated from the
raw data (see \secref{sec:twoway-fourstrat}).
<<berk-glm6>>=
coef(berk.glm3)[["dept1AG"]]
exp(coef(berk.glm3)[["dept1AG"]])
@
Finally, \figref{fig:berk-glm3-mosaic} shows the mosaic for this revised model.
The absence of shading indicates a well-fitting model.
<<berk-glm3-mosaic, fig.pos='!htb', h=6, w=6, out.width='.7\\textwidth', cap='Mosaic display for the model \\code{berk.glm3}, allowing an association of gender and admission in Department A. This model now fits the data well.', scap='Mosaic display for the model berk.glm3'>>=
mosaic(berk.glm3, shade=TRUE, formula=~Admit+Dept+Gender,
       residuals_type="rstandard", labeling=labeling_residuals,
       main="Model: [DeptGender][DeptAdmit] + DeptA*[GA]")
@

\end{Example}


%\section{Two-way tables}\label{sec:loglin-twoway}
\section{Equivalent logit models}\label{sec:loglin-logit}

Because \loglin models are  formulated as models for the
log (expected) frequency, they make no distinction between
response and explanatory variables.
In effect, they treat all variables as responses
and describe their associations.

Logit (logistic regression) models, on the other hand,
describe how the log odds for one variable depends on other,
explanatory variables.
There is a close connection between the two:
When there is a response variable, each logit model for that response
is equivalent to a \loglin model.

This relationship often provides a simpler way to formulate and test
the model, and to plot and interpret the fitted results.
Even when there is no response variable, the logit representation
for one variable helps to interpret a \loglin model in terms of
odds ratios.
The price paid for this simplicity is that associations among the
explanatory variables are not expressed in the model.

Consider, for example, the model of homogeneous association, $\LLM{AB,AC,BC}$,
\eqref{eq:lno3way} for a three-way table, and let variable $C$
be a binary response.  Under this model, the logit for variable $C$
is
\begin{eqnarray*}
  L_{ij}  =
  \log \left(  \frac{\pi_{ij|1}}{\pi_{ij|2}} \right) & = &
  \log \left(  \frac{m_{ij1}}{m_{ij2}} \right) \\
    &  = &
  \log (m_{ij1}) - \log (m_{ij2})
  \period
\end{eqnarray*}
Substituting from \eqref{eq:lno3way}, all terms which do not
involve variable $C$ cancel, and we are left with
\begin{eqnarray} \label{eq:logitab1}
  L_{ij}  =
  \log ( m_{ij1} /  m_{ij2} )  & = &
  ( \lambda_1^C - \lambda_2^C )  +
  ( \lambda_{i1}^{AC} - \lambda_{i2}^{AC} )  +
  ( \lambda_{j1}^{BC} - \lambda_{j2}^{BC} )  \nonumber \\
  &  = &
  2 \lambda_1^C   +   2 \lambda_{i1}^{AC} +   2 \lambda_{j1}^{BC} \comma
\end{eqnarray}
because all \(\lambda\) terms sum  to zero.  We are interested in how these
logits depend on $A$ and $B$, so we can simplify the notation by
replacing the $\lambda$ parameters
with more familiar ones,
 \(\alpha  = 2
\lambda_1^C\), \(\beta _i^A = 2 \lambda_{i1}^{AC}\), etc., which express this relation more directly,
\begin{equation}\label{eq:logitab2}
  L_{ij}  =
  \alpha   +  \beta _i^A +  \beta _j^B
  \period
\end{equation}
In the logit model \eqref{eq:logitab2}, the response, $C$, is affected
by both $A$ and $B$, which have additive effects on the log odds of response
category $C_1$ compared to $C_2$.
The terms $\beta _i^A$ and  $\beta _j^B$
correspond directly to $[AC]$ and $[BC]$ in the \loglin\ model \eqref{eq:lno3way}. The association among the explanatory variables,
$[AB]$ is assumed in the logit model, but this model provides no explicit
representation of that association.  The logit model \eqref{eq:logitab1}
is equivalent to the \loglin\ model $[AB] [AC] [BC]$ in goodness-of-fit
and fitted values, and parameters in the two models correspond directly.

\input{ch08/tab/loglin-logit}
\tabref{tab:loglin-logit} shows the equivalent relationships between all
\loglin and logit models for a three-way table when variable $C$ is a binary
response.  Each model necessarily includes the $\llmterm{AB}$ association
involving the predictor variables. The most basic model, $\LLM{AB,C}$,
is the intercept-only model, asserting constant odds for variable $C$.
The saturated \loglin model $\LLM{ABC}$, allows an interaction in the
effects of $A$ and $B$ on $C$, meaning that the $AC$ association or
odds ratio varies with $B$.

More generally, when there is a binary response variable, say $R$, and
one or more explanatory variables, $A, B, C, \dots$, any logit model
for $R$ has an equivalent \loglin form.
Every term in the logit model, such as $\beta_{ik}^{AC}$, corresponds to
an association of those factors with $R$, that is, $[ACR]$ in the
equivalent \loglin\ model.

The equivalent \loglin model must also include all
associations among the explanatory factors, the term $[A B C \dots]$.
Conversely, any \loglin model which includes all associations among
the explanatory variables has an equivalent logit form.
When the response factor has more than two categories, models for
generalized logits (\secref{sec:genlogit}) also
have an equivalent \loglin form.

\begin{Example}[berkeley7]{Berkeley admissions}
The homogeneous association model, $[AD] [AG] [DG]$
did not fit the \data{UCBadmissions} data very well,
and we saw that the term $[AG]$ was unnecessary.
Nevertheless, it is instructive to consider the equivalent logit model.
We illustrate the features of the logit model which lead to the same
conclusions and simplified interpretation from graphical displays.

Because Admission
is a binary response variable, model \eqref{eq:berk1} is equivalent
to the logit model,
\begin{equation}\label{eq:berk3}
  L_{ij} =
  \log \left(
  \frac{m_{ \mbox{\scriptsize{Admit}} (ij) }} {m_{ \mbox{\scriptsize{Reject}} (ij) }}
  \right)
  =
  \alpha   +  \beta _i^{\mbox{\scriptsize Dept}}
  +  \beta _j^{\mbox{\scriptsize Gender}}
  \period
\end{equation}
That is, the logit model \eqref{eq:berk3} asserts that department and
gender have additive effects on the log odds of admission.
A significance test for the term $\beta _j^{\mbox{\scriptsize Gender}}$
here is equivalent to the test
of the $[AG]$ term for gender bias in the \loglin model.
The observed log odds of admission here can be calculated as:
<<berk-logit1, R.options=list(digits=4)>>=
(obs <- log(UCBAdmissions[1,,] / UCBAdmissions[2,,]))
@


With the data in the form of the frequency data frame \code{berkeley}
we used in \exref{ex:berkeley6}, the logit model \eqref{eq:berk3}
can be fit using \func{glm} as shown below.  In the model formula,
the binary response is \code{Admit=="Admitted"}. The \code{weights}
argument gives the frequency, \code{Freq} in each table cell.
\TODO{This gives the same fitted values, but not the same LR tests. Why?}

<<berk-logit2>>=
berk.logit2 <- glm(Admit=="Admitted" ~ Dept + Gender,
                   data=berkeley, weights=Freq, family="binomial")
summary(berk.logit2)
@

As in logistic regression models, parameter estimates may be interpreted
as increments in the log odds, or $\exp(\beta)$ may be interpreted
as the multiple of the odds associated with the explanatory categories.
Because \func{glm} uses a baseline category parameterization (by default)
the coefficients of the first category of \code{Dept} and
\code{Gender} are set to zero.
You can see from the \func{summary} output
that the coefficients for the departments decline steadily
from A--F.%
\footnote{
In fact, the departments were labeled A--F in decreasing order of
rate of admission.
}
The coefficient $\beta _F^{\mbox{\scriptsize Gender}} = 0.0999$
for females indicates that, overall, women were $\exp({0.0999}) = 1.105$
times as likely as male applicants to be admitted to graduate school
at U.C. Berkeley, a 10\% advantage.

Similarly, the logit model equivalent of the \loglin model \eqref{eq:berk2}
\code{berk.glm3}
containing the extra 1 df term for an effect of gender in Department A is
\begin{equation}\label{eq:berk4}
  L_{ij} =
  \alpha   +  \beta _i^{\mbox{\scriptsize Dept}}
  +  I(j=1) \beta^{\mbox{\scriptsize Gender}}
  \period
\end{equation}
This model can be fit as follows:
<<berk-logit3>>=
berkeley <- within(berkeley,
                   dept1AG <- (Dept=='A')*(Gender=='Female'))
berk.logit3 <- glm(Admit=="Admitted" ~ Dept + Gender + dept1AG,
                   data=berkeley, weights=Freq, family="binomial")
@
In contrast to the tests for individual coefficients, the \func{Anova}
method in the \Rpackage{car} gives \LR tests of the terms in a model.
As mentioned earlier, this provides \emph{partial}
(``Type II'') tests for the additional contribution of each term beyond
all others.
%\TODO{Fix: Too few digits are shown here for LR chisq}
<<berk-anova, R.options=list(digits=6)>>=
library(car)
Anova(berk.logit2)
Anova(berk.logit3)
@


\subsubsection*{Plotting logit models}
Logit models are easier to interpret than the corresponding \loglin
models because there are fewer parameters,
and because these parameters pertain to the odds of a response category
rather than to cell frequency.
Nevertheless, interpretation is often easier still from a graph than from the
parameter values.

The simple interpretation of these logit models can be seen by plotting the
logits for a given model.
To do that, it is necessary to construct a data frame
containing the observed (\code{obs}) and fitted (\code{fit}) for the
combinations of gender and department.
<<berk-pred2>>=
pred2 <- cbind(berkeley[,1:3], fit=predict(berk.logit2))
pred2 <- cbind(subset(pred2, Admit=="Admitted"), obs=as.vector(obs))
head(pred2)
@

In this form, these results can be plotted as a line plot of the
fitted logits vs.\ department, with separate curves for males
and females, and adding points to show the observed values.
Here, we use \pkg{ggplot2} as shown below, with
the \func{aes} arguments \code{group=Gender, color=Gender}.
This produces the left panel in \figref{fig:berk-logit}.
The same steps for the model \code{berk.logit3}
gives the right panel in this figure.
The observed logits, of course, are the same in both plots.

<<berk-logit-plot, eval=FALSE, fig.show='hide'>>=
library(ggplot2)
ggplot(pred2, aes(x=Dept, y=fit, group=Gender, color=Gender)) +
  geom_line(size=1.2) +
  geom_point(aes(x=Dept, y=obs, group=Gender, color=Gender), size=4) +
  ylab("Log odds (Admitted)") + theme_bw() +
  theme(legend.position=c(.8, .9),
        legend.title=element_text(size=14),
        legend.text=element_text(size=14))
@


\begin{figure}[!htb]
\centering
\includegraphics[width=0.49\textwidth]{ch08/fig/berk-logit2}
\includegraphics[width=0.49\textwidth]{ch08/fig/berk-logit3}
\caption{Observed (points) and fitted (lines) log odds of admissions in the logit models for
the \data{UCBAdmissions} data. Left: the logit model \eqref{eq:berk3} corresponding to the
\loglin model [AD] [AG] [DG]. Right: the logit model \eqref{eq:berk4}, allowing only a 1 df
term for Department A.}
\label{fig:berk-logit}
\end{figure}

The effects seen in our earlier analyses (Examples~\ref{ex:berkeley4},
\ref{ex:berkeley4b} and \ref{ex:berkeley6})
may all be observed in these plots.
In the left panel of \figref{fig:berk-logit},
corresponding to the \loglin model $\LLM{AD, AG, DG}$,
the effect of gender,  $\beta _j^{\mbox{\scriptsize Gender}}$,
in the equivalent logit model
is shown by the constant separation
between the two curves.
From the plot we see that this effect is very small (and nonsignificant).
In the right panel, corresponding to the logit model \eqref{eq:berk4},
there is no effect of gender on admission, except in department A, where the
extra parameter allows perfect fit.


\end{Example}

\section{Zero frequencies}\label{sec:loglin-zeros}

Cells with frequencies of zero create problems for \loglin and logit
models.  For \loglin models, most of the derivations of
expected
frequencies by maximum likelihood
and other quantities that depend on these (e.g., \GSQ tests)
assume that all $n_{ijk\cdots} > 0$.
In analogous
logit models, the observed log odds (e.g., for a three-way table),
$\log ( n_{ij1} / n_{ij2} )$, will be undefined if either frequency is
zero.

Zero frequencies may occur in \ctabs for two different reasons:
\begin{itemize}
\item \term{structural zeros} (also called \emph{fixed zeros}) will occur when it is impossible to observe
values for some combinations of the variables.
For these cases we should have $\widehat{m}_i$ = 0 wherever $n_i=0$.
For example, suppose we have three different methods of contacting
people at risk for some obscure genetically inherited disease:
newspaper advertisement, telephone campaign, and radio appeal.
If each person contacted in any way is classified dichotomously
by the three methods of contact, there can never be a non-zero
frequency in the `No-No-No' cell.%
\footnote{Yet, if we fit an unsaturated model, expected frequencies
may be estimated for all cells, and provide a means to estimate
the total number at risk in the population.
See \citet[Section 5.4]{Lindsey:95}.}
Similarly, in a tabulation of seniors by
gender and health concerns, there
can never be males citing menopause or females citing prostate cancer.
Square tables, such as wins and losses for sporting teams often have
structural zeros in the main diagonal.

\item \term{sampling zeros} (also called \emph{random zeros})
occur when the total size of the sample is not large enough in relation to the probabilities in each of the cells to assure that someone will be observed
in every cell. Here, it is permissible to have $\widehat{m}_i$ > 0 when $n_i=0$.
This problem increases with the number of table variables.
For example, in a European survey of religious affiliation, gender and occupation,
we may not happen to observe any female Muslim vineyard-workers in France, although such individuals surely exist in the population.
Even when zero frequencies do not occur, tables with many cells relative to
the total frequency tend to produce small expected frequencies in at
least some cells, which tends to make  the \GSQ statistics for model fit
and \LR statistics for individual terms unreliable.
\end{itemize}

Following \citet{Birch:1963}, \citet{Haberman:74} and many others \citep[e.g.,][]{Bishop-etal:75}
identified conditions under which the maximum likelihood estimate for
a given \loglin model does not exist, meaning that the algorithms used in
\func{loglin} and \func{glm} do not converge to a solution.
The problem depends on the number and locations of the zero cells, but not on the
size of the frequencies in the remaining cells.
\citet{FienbergAlessandro:2007} give a historical overview of the problem
and current approaches and \citet[\S 10.6]{Agresti:2013} gives a compact
summary.

In \R, the mechanism to handle structural zeros in the IPF approach of
\func{loglin} and \func{loglm} is to supply the argument \code{start},
giving a table conforming to the data, containing values of 0 in
the locations of the zero cells, and non-zero elsewhere.%
\footnote{
If structural zeros are present, the calculation of degrees of freedom may not be correct. \func{loglm} deducts one degree of freedom for each structural zero, but cannot make allowance for
patterns of zeros based on the fitted margins that lead to
gains in degrees of freedom due to smaller dimension in the parameter space.
\func{loglin} makes no such correction.
}
In the \func{glm} approach, the argument \code{subset=Freq > 0} can be
used to remove the cells with zero frequencies from the data,
or else, zero frequencies can be set to \code{NA}.
This usually  provides the correct degrees of freedom, however some estimated
coefficients may be infinite.

For a complete table, the residual degrees of freedom are determined as
\begin{equation*}
df = \mbox{\# of cells} - \mbox{\# of fitted parameters}
\end{equation*}
For tables with structural zeros, an analogous general formula is
\begin{equation}\label{eq:dfzeros}
df = (\mbox{\# cells} - \mbox{\# of parameters})
   - (\mbox{\# zero cells} - \mbox{\# of NA parameters})
\end{equation}
where NA parameters refers to parameters that cannot be estimated
due to zero marginal totals in the model formula.


In contrast, sampling zeros are often handled by some modification of the
data frequencies to ensure all non-zero cells.
Some suggestions are:
\begin{itemize*}
\item Add a small positive quantity (0.5 is often recommended) to \emph{every}
cell in the \ctab \citep{Goodman:70}, as is often done in calculating
empirical log odds (\exref{ex:toxaemia}); this simple approach over-smooths the
data for unsaturated models, and should be deprecated, although widely
used in practice.

\item Replace sampling zeros by some small number, typically
$10^{-10}$ or smaller \citep{Agresti:90}.
\item Add a small quantity, like 0.1, to \emph{all} zero cells, sampling or structural
\citep{EversNamboodiri:77}.
\end{itemize*}
In complex, sparse tables, a sensitivity analysis, comparing different approaches can help determine if the substantive conclusions vary with the approach to zero cells.

\begin{Example}[health]{Health concerns of teenagers}
\citet[Table 8-3]{Fienberg:80} presented a classic example of structural
zeros in the analysis of the $4 \times 2 \times 2$
table shown in \tabref{tab:health}.  The data come from a survey of
health concerns among teenagers, originally from \citet{Brunswick:1971}.
Among the health concerns, the two zero entries for menstrual problems
among males are clearly structural zeros and there therefore one
structural zero in the concern by gender marginal table.
As usual, we abbreviate the table variables concern, age, gender
by their initial letters, C, A, G below.

\input{ch08/tab/health}

The \data{Health} data is created as a frequency data frame as follows.
<<health1>>=
Health <- expand.grid(concerns = c("sex", "menstrual",
                                   "healthy", "nothing"),
                      age      = c("12-15", "16-17"),
                      gender   = c("M", "F"))
Health$Freq <- c(4, 0, 42, 57, 2, 0, 7, 20,
                 9, 4, 19, 71, 7, 8, 10, 21)
@
In this form, we first use \func{glm} to fit two small models,
neither of which involves the $\{C G\}$ margin.
Model \code{health.glm0} is the model of mutual independence, $\LLM{C,A,G}$.
Model \code{health.glm1} is the model of joint independence, $\LLM{C,AG}$,
allowing an association between age and gender, but neither with concern.
As noted above, the argument \code{subset=(Freq>0)} is used to
eliminate the structural zero cells.

<<health2>>=
health.glm0 <-glm(Freq ~  concerns + age + gender, data=Health,
                  subset=(Freq>0), family=poisson)
health.glm1 <-glm(Freq ~  concerns + age * gender, data=Health,
                  subset=(Freq>0), family=poisson)
@
Neither of these fits the data well.  To conserve space, we show only the
results of the \GSQ tests for model fit.
<<health3>>=
vcdExtra::Summarise(health.glm0, health.glm1)
@
To see why, \figref{fig:health-mosaic} shows the mosaic display for model
\code{health.glm1}, $\LLM{C, AG}$.  Note that \func{mosaic}
takes care to make cells of zero frequency more visible by
marking them with a small ``o'', as these have an area of zero.

<<health-mosaic, h=6, w=7, out.width='.6\\textwidth', cap='Mosaic display for the Health data, model \\code{health.glm1}', scap='Mosaic display for the Health data, model health.glm1'>>=
mosaic(health.glm1, ~concerns+age+gender, residuals_type="rstandard")
@
This suggests that there are important associations at least between concern and gender
($[CG]$) and between concern and age ($[CA]$).
These are incorporated into the next model:
<<health4>>=
health.glm2 <-glm(Freq ~  concerns*gender + concerns*age, data=Health,
                  subset=(Freq>0), family=poisson)
vcdExtra::Summarise(health.glm2)
@
The degrees of freedom are correct here.  \eqref{eq:dfzeros},
with 2 zero cells and 1 NA parameter due to the zero in the $\{CG\}$ margin
gives $df = (16-12) - (2-1) = 3$.  The loss of one estimable parameter
can be seen in the output from \code{summary}.
<<health5, size='footnotesize', R.options=list(width=80)>>=
summary(health.glm2)
@

In contrast, \func{loglm} reports the degrees of freedom incorrectly for
models containing zeros in any fitted margin.
For use with \func{loglm},  we convert it to a $4 \times 2 \times$ table.
<<health-loglin1>>=
health.tab <- xtabs(Freq ~ concerns + age + gender, data = Health)
@
The same three models are fitted with \func{loglm} as shown below.
The locations of the positive frequencies are marked in the
array \code{nonzeros} and supplied as the value of the
\code{start} argument. 
%\TODO{Check whether this will work with Summarise()}
<<health-loglm2>>=
nonzeros <- ifelse(health.tab>0, 1, 0)
health.loglm0 <- loglm(~ concerns + age + gender,
              data = health.tab, start = nonzeros)
health.loglm1 <- loglm(~ concerns + age * gender,
              data = health.tab, start = nonzeros)
# df is wrong
health.loglm2 <- loglm(~ concerns*gender + concerns*age,
              data = health.tab, start = nonzeros)
Summarise(health.loglm0, health.loglm1, health.loglm2)
@
% \TODO{Test bug fix with Summarise()}
% <<health-loglm2-test>>=
% Summarise(health.loglm0, health.loglm1, health.loglm2)
% @

The results agree with those of \func{glm}, except for the degrees of freedom
for the last model.

\end{Example}



\section{Models for ordinal variables}\label{sec:loglin-ordinal}
<<ordinal,  child='ch08/ordinal.Rnw'>>=
@
 
\section{Square tables}\label{sec:loglin-square}
<<square, child='ch08/square.Rnw'>>=
@

\section{Three-way and higher-dimensional tables}\label{sec:loglin-3wayord}

The models and methods for ordinal factors and square tables described in
\secref{sec:loglin-ordinal} and \secref{sec:loglin-square} extend readily
to multidimensional tables with these properties for some of the factors.
In three-way tables, these models provide a more parsimonious account
than the saturated model, $\LLM{ABC}$, and also allow simpler models
than the general model of homogeneous association, $\LLM{AB,AC,BC}$ using
scores for ordinal factors or terms for symmetry and diagonal factors
in square layers.

For example, consider the case where all three factors are ordinal and
the model of homogeneous association $\LLM{AB,AC,BC}$ fits poorly.
In this case we can generalize the model of uniform association by
assigning scores $\vec{a}$, $\vec{b}$ and $\vec{c}$
and model the three-way association,
$\lambda_{ijk}^{ABC}$ as
\begin{equation*}
\lambda_{ijk}^{ABC} = \gamma a_i b_j c_k
\end{equation*}
with only one more parameter. This gives the model of
\term{uniform interaction} (or \emph {homogeneous uniform association})
\begin{equation} \label{eq:uni-inter}
  \log  (m_{ijk})  =
  \mu  +  \lambda_i^A
  +  \lambda_j^B
  +  \lambda_k^C
  +  \lambda_{ij}^{AB}
  +  \lambda_{ik}^{AC}
  +  \lambda_{jk}^{BC}
  +  \gamma a_i b_j c_k
  \period
\end{equation}
This model posits that (with equally spaced scores) all local odds ratios
$\theta_{ijk}$ in adjacent rows, columns and layers are constant,
\begin{equation*}
\log (\theta_{ijk}) = \gamma  \quad\quad \forall \quad i, j, k
\end{equation*}
The homogeneous association model is the special case of $\log \theta_{ijk} = \gamma = 0$.

A less restricted model of \term{heterogeneous uniform association}
retains the linear-by-linear form of association for
factors $A$ and $B$, but allows the strength of this association to vary over
layers, $C$, representing
$\lambda_{ijk}^{ABC}$ as
\begin{equation*}
\lambda_{ijk}^{ABC} = (\gamma + \gamma_k) a_i b_j
\end{equation*}
with the constraint $\sum_k \gamma_k =0$.  This model is equivalent to fitting
separate models of uniform association at each level $k$ of factor $C$
and gives estimates of the conditional local log odds ratios,
$\log \theta_{ij(k)} = \gamma + \gamma_k$.

Following the development in \secref{sec:loglin-ordinal} there is a large
class of other models for ordinal factors (see \figref{fig:assoc-models}),
where not all factors are assigned scores.
For three-way tables, these can be represented in homogeneous form
when the two-way association of $A$ and $B$ is the same for all levels
of $C$, or in a heterogeneous form, when it varies over $C$.

Similarly, the models for square tables described in \secref{sec:loglin-square}
extend to three-way tables with several layers (strata), allowing both homogeneous and
heterogeneous terms for diagonals and symmetry
describing the $AB$ association over levels of $C$.

\begin{Example}[vision-glm2]{Visual acuity}
We continue the analysis of the \data{VisualAcuity} data,
but now consider the three-way, $4 \times 4\times 2$
table comprising both men and women.  The main questions
here are whether the pattern of quasi-symmetry observed in the
analysis for women also pertains to men and whether there
is heterogeneity of the association between right, left acuity
across gender.

A useful first step for $n$-dimensional tables is to consider
the models composed of all 1-way, 2-way, $\dots n$-way terms
as a quick overview.  The function \func{Kway} in \Rpackage{vcdExtra}
does this automatically, returning a \class{glmlist} object
containing the fitted models.%
\footnote{
For completeness, this also fits the 0-way model, corresponding to
$\log m_{ijk\dots} = \mu$, or the model formula \code{Freq} $\sim$ \code{1}.
}

<<vision2-kway>>=
vis.kway <-Kway(Freq ~ right + left + gender, data=VisualAcuity)
vcdExtra::Summarise(vis.kway)
@
This shows that the model of homogeneous association \code{kway.2}
($\LLM{RL, RG, LG}$) does not fit well, but it doesn't account
for diagonal agreement or symmetry to simplify the associations.

As a basis for comparison, we first fit the simple models of
quasi-independence and quasi-symmetry that do not involve \var{gender},
asserting the same pattern of diagonal and off-diagonal cells for males
and females.
<<vision2-glm1>>=
vis.indep <- glm(Freq ~ right + left + gender,  data = VisualAcuity,
                 family=poisson)
vis.quasi <- update(vis.indep, . ~ . + Diag(right, left))
vis.qsymm <- update(vis.indep, . ~ . + Diag(right, left) + Symm(right, left))

Summarise(vis.indep, vis.quasi, vis.qsymm)
@
The model of homogeneous quasi-symmetry fits quite badly, even worse than the all two-way
association model.  We can see why in the mosaic for this model, shown in \figref{fig:vision2-qsymm}.
<<vision2-qsymm, h=6, w=6, out.width='.7\\textwidth', cap='Mosaic display for the model of homogeneous quasi-symmetry fit to the VisualAcuity data.'>>=
mosaic(vis.qsymm, ~ gender + right + left, condvars="gender",
       residuals_type="rstandard", gp=shading_Friendly,
       labeling_args=largs,
       main="Homogeneous quasi-symmetry")
@
It can be seen in \figref{fig:vision2-qsymm} that the pattern of residuals for men and women are nearly completely opposite
in the upper and lower portions of the plot: men have positive residuals in the same
\code{right}, \code{left} cells where women have negative residuals, and vice-versa.
In particular, the diagonal cells of both tables have large absolute residuals, because
the term \code{Diag(right, left)} fits a common set of diagonals for both men and women.

We can correct for this by allowing separate diagonal and symmetry terms, given
as interactions of \code{gender} with \func{Diag} and \func{Symm}.

<<vision2-glm2>>=
vis.hetdiag <- update(vis.indep, . ~ . + gender*Diag(right, left) +
                      Symm(right, left))
vis.hetqsymm <- update(vis.indep, . ~ . + gender*Diag(right, left) +
                       gender*Symm(right, left))
#vis.hetmodels <- glmlist(vis.qsymm, vis.hetdiag, vis.hetqsymm)
Summarise(vis.qsymm, vis.hetdiag, vis.hetqsymm)
@
\noindent Note that the model \code{vis.hetqsymm} fits better than the model \code{vis.hetdiag}
in absolute terms and by AIC, but the latter, with fewer parameters, fits better by BIC.
The mosaic for the model \code{vis.hetqsymm} is shown in \figref{fig:vision2-hetqsymm}.
<<vision2-hetqsymm, h=6, w=6, out.width='.7\\textwidth', cap='Mosaic display for the model of heterogeneous quasi-symmetry fit to the VisualAcuity data.'>>=
mosaic(vis.hetqsymm, ~ gender + right + left, condvars="gender",
       residuals_type="rstandard", gp=shading_Friendly,
       labeling_args=largs,
       main="Heterogeneous quasi-symmetry")
@
As in the two-way case, this model now fits the diagonal cells in each table exactly,
effectively ignoring this part of the association between right and left eye
acuity. All remaining residuals are relatively small in magnitude, except for the
two opposite off-diagonal cells \code{(Low, High)} and \code{(High, Low)} in the
table for women.

The substantive interpretation of this example is that
visual acuity is largely the same (diagonal cells)
in the right and left eyes of both men and women.  Ignoring the diagonal cells,
when visual acuity differs, both men and women exhibit approximately symmetric
associations.  However, deviations from symmetry (\figref{fig:vision2-qsymm})
are such that men are slightly more likely to have a lower grade in the right eye,
while women are slightly more likely to have a higher grade in the right eye.


%\TODO{Complete this example with models and plots}

\end{Example}

%\section{An extended example}\label{sec:loglin-vietnam}

%\section{Influence and diagnostic plots for \loglin\ models}\label{sec:loglin-infl}

\section{Multivariate responses}\label{sec:loglin-multiv}

<<multiv, child='ch08/multiv.Rnw'>>=
@

<<coalminers-ex, child='ch08/coalminers.Rnw'>>=
@

\subsection{More complex models}
When there is more than one explanatory variable and several responses,
the methods described above using \func{glm} and \func{vglm} still
apply.  However, it is useful to begin with a more thorough
visual examination of the relations within and between these sets.
Some useful graphical displays include:
\begin{itemize}
\item mosaic displays showing the marginal relations among the response variables
and of the explanatory variables, each collapsed over the other set;
\item conditional mosaics or fourfold displays of the associations among
the responses, stratified by one or more of the explanatory variables;
\item plots of empirical logits and log odds ratios, as in \figref{fig:cm-blogits}
or model-based plots, such as \figref{fig:cm-vglm2-blogit}, showing a model-smoothed
summary.
\end{itemize}
These displays can, and should, inform our search for an adequate
descriptive or explanatory model.  Some of these ideas are illustrated in the following
example.

<<tox-ex, child='ch08/toxaemia.Rnw'>>=
@




\section{Chapter summary}\label{sec:loglin-summary}
\input{ch08/summary}

\section{Further reading}\label{sec:loglin-reading}

\section{Lab exercises}\label{sec:loglin-lab}
<<exercises08, child="ch08/exercises.Rnw">>=


<<cleanup8, size='footnotesize'>>=
#detach(package:corrplot)
detach(package:VGAM)
#detach(package:logmult)
#remove(list=objects(pattern="\\.tab|\\.df|\\.fit"))
.locals$ch08 <- setdiff(ls(), .globals)
#.locals$ch08
#remove(list=.locals$ch08[sapply(.locals$ch03,function(n){!is.function(get(n))})])
@

